{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TheAgaveFairy/MusicGeneration/blob/main/gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTPZ89mS-YuW"
   },
   "source": [
    "https://www.kaggle.com/code/arnavmishra6996/music-generation-using-decoder-only-model-gpt2\n",
    "\n",
    "this is the inspiration, more or less copying it but in my own typing + modifications so i understand it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8RS4Y2uM-V0e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import transformers\n",
    "\n",
    "from helpers import list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(midi_df):\n",
    "    TRANSPOSE_RANGE = 3\n",
    "    TEMPO_RANGE = 0.15\n",
    "    VELOCITY_RANGE = 5\n",
    "\n",
    "    transpose = np.random.randint(-TRANSPOSE_RANGE, TRANSPOSE_RANGE)\n",
    "    tempo = np.random.uniform(1 - TEMPO_RANGE, 1 + TEMPO_RANGE)\n",
    "    velocity = np.random.randint(-VELOCITY_RANGE, VELOCITY_RANGE)\n",
    "    #print(f\"trans: {transpose}, tempo: {tempo}\")\n",
    "\n",
    "    if np.random.random() > 0.25:\n",
    "        print(f\"HRT time: transing {transpose} steps\")\n",
    "        df['note_value'] = df['note_value'] + transpose\n",
    "        \n",
    "    if np.random.random() > 0.25:\n",
    "        print(f\"codeine time: tempo change {tempo}\")\n",
    "        df['start_time'] = df['start_time'] * tempo\n",
    "        df['end_time'] = df['end_time'] * tempo\n",
    "\n",
    "    if np.random.random() > 0.25:\n",
    "        print(f\"bad musician: velocity {velocity}\")\n",
    "        df['velocity'] = df['velocity'] + velocity\n",
    "\n",
    "    return df # but isnt this already doing it in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator():\n",
    "    \n",
    "    yield midi_as_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_folder_path = './inputs/final/Debussy'\n",
    "test_file_name = os.listdir(test_folder_path)[0]\n",
    "test_file_path = os.path.join(test_folder_path, test_file_name)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(os.listdir(test_folder_path))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(test_file_path)\n",
    "print(df.head())\n",
    "\n",
    "augment_data(df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4dFQv20VCyI"
   },
   "source": [
    "GPT-2 needs this as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zJaDtaAyVB8V"
   },
   "outputs": [],
   "source": [
    "def tokenize_midi(midi_df):\n",
    "    \"\"\"\n",
    "    with open('midi_text_data.txt', 'w') as file:\n",
    "        file.write('<|startoftext|>\\n')\n",
    "        for index, row in tqdm(df.iterrows()):\n",
    "            text_line = f\"start_time: {row['start_time']}, end_time: {row['end_time']}, instrument: {row['instrument']}, note: {row['note']}, start_beat: {row['start_beat']}, end_beat: {row['end_beat']}, note_value: {row['note_value']}, duration: {row['duration']}, velocity: {row['velocity']}\\n\"\n",
    "            file.write(text_line)\n",
    "        file.write('<|endoftext|>\\n')\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3gTDQt3VNqo"
   },
   "source": [
    "Here we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktVqn5b0VOoa",
    "outputId": "b0db262f-0c7f-4471-bae7-a1ac0e4ee14c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 14:58:57.025290: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-03 14:58:57.116639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743710337.153922   75106 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743710337.164525   75106 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-03 14:58:57.250726: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "I0000 00:00:1743710341.155452   75106 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5147 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def load_tf_dataset(file_path, tokenizer, block_size=128, chunk_size=1000000):\n",
    "    def chunk_generator():\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            while True:\n",
    "                chunk = f.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                yield chunk\n",
    "\n",
    "    def process_chunk(chunk):\n",
    "        tokens = tokenizer.tokenize(chunk)\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        for i in range(0, len(ids) - block_size + 1, block_size):\n",
    "            input_ids = ids[i:i+block_size]\n",
    "            labels = ids[i+1:i+block_size+1] # shifted by 1\n",
    "            labels = [-100 if token == tokenizer.pad_token_id else token for token in labels] # masking with -100\n",
    "\n",
    "            yield {\n",
    "                \"input_ids\": input_ids,#ids[i:i+block_size],\n",
    "                \"attention_mask\": [1] * block_size,\n",
    "                \"labels\": labels\n",
    "            }\n",
    "\n",
    "    def gen():\n",
    "        for chunk in tqdm(chunk_generator(), desc=\"Processing chunks\"):\n",
    "            yield from process_chunk(chunk)\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            {\n",
    "                \"input_ids\": tf.TensorSpec(shape=(block_size,), dtype=tf.int32),\n",
    "                \"attention_mask\": tf.TensorSpec(shape=(block_size,), dtype=tf.int32),\n",
    "                \"labels\": tf.TensorSpec(shape=(block_size,), dtype=tf.int32)\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = load_tf_dataset('midi_text_data.txt', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyzBvavLRQEy",
    "outputId": "c20e9e6c-b05d-44bd-8342-86c2b687421e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')],\n",
       " [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('CPU'),tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jm3TC7mjl9rV"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, TFTrainingArguments #TFTrainer,\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TFTrainer# as TFTrainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir=\"./gpt2-midi\",  # Directory where the model and tokenizer will be saved\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size for training\n",
    "    save_steps=10_000,  # Save every 10,000 steps\n",
    "    save_total_limit=2,  # Keep only the 2 most recent models\n",
    "    prediction_loss_only=True,  # Only compute the loss during training\n",
    "    logging_dir='./logs',  # Directory for logs\n",
    "    logging_steps=500  # Log every 500 steps\n",
    ")\n",
    "\n",
    "#Gemini\n",
    "strategy = tf.distribute.MirroredStrategy(['CPU:0'])\n",
    "\n",
    "# Create the model within the strategy scope\n",
    "with strategy.scope():\n",
    "    model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    #/Gemini\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = TFTrainer(\n",
    "    model=model,#TFGPT2LMHeadModel.from_pretrained('gpt2'),\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\"\"\"\n",
    "'This code was deprecated on or about 2021.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmNXNj64l-R5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "trainer.save_model('./gpt2-midi')\n",
    "tokenizer.save_pretrained('./gpt2-midi')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fmYZs41_UZ_q",
    "outputId": "7865b3b3-5fa0-4a84-ac3b-08d693bd6a63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 10:02:30.299578: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-14 10:02:30.391479: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739548950.431066   11259 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739548950.442127   11259 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-14 10:02:30.530080: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1739548952.842162   11259 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5616 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 67\u001b[0m\n\u001b[1;32m     63\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[1;32m     66\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m prepare_dataset_for_training(\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mtrain_dataset\u001b[49m,  \u001b[38;5;66;03m# Your original train_dataset\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     config\u001b[38;5;241m.\u001b[39mper_device_train_batch_size\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Custom callback for model saving\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mModelCheckpoint\u001b[39;00m(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mCallback):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#Claude 3.5Sonnet to the rescue, maybe\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "from transformers import DataCollatorForLanguageModeling, DefaultDataCollator\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Initialize tokenizer and data collator\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\"\"\"\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True # changed from False\n",
    ")\n",
    "\"\"\"\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# Training configuration\n",
    "class TrainingConfig:\n",
    "    output_dir = \"./gpt2-midi\"\n",
    "    num_train_epochs = 3\n",
    "    per_device_train_batch_size = 4\n",
    "    save_steps = 10_000\n",
    "    save_total_limit = 2\n",
    "    logging_dir = './logs'\n",
    "    logging_steps = 500\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "os.makedirs(config.logging_dir, exist_ok=True)\n",
    "\n",
    "# Set up distributed training strategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "# Create the model within the strategy scope\n",
    "with strategy.scope():\n",
    "    model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    # Set up the optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=model.compute_loss\n",
    "    )\n",
    "\n",
    "# Prepare the dataset\n",
    "def prepare_dataset_for_training(dataset, batch_size):\n",
    "    \"\"\"Convert dataset to tf.data.Dataset format\"\"\"\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    \"\"\"\n",
    "    # Modify attention mask shape\n",
    "    dataset = dataset.map(lambda x: {\n",
    "        **x,\n",
    "        \"attention_mask\": tf.reshape(x[\"attention_mask\"], (tf.shape(x[\"attention_mask\"])[0], 1, 1, tf.shape(x[\"attention_mask\"])[2]))  # Reshape to (batch_size, 1, 1, seq_len)\n",
    "    })\n",
    "    \"\"\"\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = prepare_dataset_for_training(\n",
    "    train_dataset,  # Your original train_dataset\n",
    "    config.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "# Custom callback for model saving\n",
    "class ModelCheckpoint(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, save_steps, output_dir, save_total_limit):\n",
    "        super().__init__()\n",
    "        self.save_steps = save_steps\n",
    "        self.output_dir = output_dir\n",
    "        self.save_total_limit = save_total_limit\n",
    "        self.step = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.step += 1\n",
    "        if self.step % self.save_steps == 0:\n",
    "            # Save the model\n",
    "            checkpoint_dir = os.path.join(\n",
    "                self.output_dir,\n",
    "                f\"checkpoint-{self.step}\"\n",
    "            )\n",
    "            self.model.save_pretrained(checkpoint_dir)\n",
    "\n",
    "            # Remove old checkpoints if exceeding limit\n",
    "            checkpoints = sorted([\n",
    "                d for d in os.listdir(self.output_dir)\n",
    "                if d.startswith(\"checkpoint-\")\n",
    "            ])\n",
    "            while len(checkpoints) > self.save_total_limit:\n",
    "                checkpoint_to_remove = os.path.join(\n",
    "                    self.output_dir,\n",
    "                    checkpoints.pop(0)\n",
    "                )\n",
    "                tf.io.gfile.rmtree(checkpoint_to_remove)\n",
    "\n",
    "# Set up callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        save_steps=config.save_steps,\n",
    "        output_dir=config.output_dir,\n",
    "        save_total_limit=config.save_total_limit\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=config.logging_dir,\n",
    "        update_freq=config.logging_steps\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=config.num_train_epochs,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save the final model and tokenizer\n",
    "model.save_pretrained(config.output_dir)\n",
    "tokenizer.save_pretrained(config.output_dir)\n",
    "print(f\"Model saved to {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMBu5O++ygoL0a8AYABGRuy",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
