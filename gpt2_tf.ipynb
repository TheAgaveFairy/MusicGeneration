{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TheAgaveFairy/MusicGeneration/blob/main/gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTPZ89mS-YuW"
   },
   "source": [
    "https://www.kaggle.com/code/arnavmishra6996/music-generation-using-decoder-only-model-gpt2\n",
    "\n",
    "this is the inspiration, more or less copying it but in my own typing + modifications so i understand it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8RS4Y2uM-V0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helpers import list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIDI_TEXT_FILENAME = './inputs/midi_text_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(midi_df):\n",
    "    TRANSPOSE_RANGE = 3\n",
    "    TEMPO_RANGE = 0.15\n",
    "    VELOCITY_RANGE = 5\n",
    "\n",
    "    transpose = np.random.randint(-TRANSPOSE_RANGE, TRANSPOSE_RANGE)\n",
    "    tempo = np.random.uniform(1 - TEMPO_RANGE, 1 + TEMPO_RANGE)\n",
    "    velocity = np.random.randint(-VELOCITY_RANGE, VELOCITY_RANGE)\n",
    "    #print(f\"trans: {transpose}, tempo: {tempo}\")\n",
    "\n",
    "    if np.random.random() > 0.25:\n",
    "        print(f\"HRT time: transing {transpose} steps\")\n",
    "        df['note_value'] = df['note_value'] + transpose\n",
    "        \n",
    "    if np.random.random() > 0.25:\n",
    "        print(f\"codeine time: tempo change {tempo}\")\n",
    "        df['start_time'] = df['start_time'] * tempo\n",
    "        df['end_time'] = df['end_time'] * tempo\n",
    "\n",
    "    if np.random.random() > 0.25:\n",
    "        print(f\"bad musician: velocity {velocity}\")\n",
    "        df['velocity'] = df['velocity'] + velocity\n",
    "\n",
    "    return df # but isnt this already doing it in place idk honestly what best practice here is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_loader(path):\n",
    "    \"\"\"\n",
    "    Returns a clean Pandas DataFrame\n",
    "    I just want to do clean up the API, do type forcing etc\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(test_file_path)\n",
    "    #df = df.astype({\"note_value\": int, \"velocity\": int, \"instrument_program_number\": int})\n",
    "    for c in df:\n",
    "        print(c, df[c].dtype)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder_path = './inputs/final/Debussy'\n",
    "test_file_name = os.listdir(test_folder_path)[0]\n",
    "test_file_path = os.path.join(test_folder_path, test_file_name)\n",
    "\n",
    "#dataset = tf.data.Dataset.from_tensor_slices(os.listdir(test_folder_path))\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = csv_loader(test_file_path)\n",
    "print(df.head())\n",
    "\n",
    "augment_data(df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4dFQv20VCyI"
   },
   "source": [
    "GPT-2 needs this as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJaDtaAyVB8V"
   },
   "outputs": [],
   "source": [
    "def tokenize_midi(df, header, filename = None):\n",
    "    \"\"\"\n",
    "    Takes in a DataFrame from a csv we made from a midi file, and turns it into GPT-2 friendly text.\n",
    "    Saves it as a text file if desired and returns it as a String\n",
    "    \"\"\"\n",
    "    \n",
    "    representation = []\n",
    "    representation.append(f'<|startofpiece|> {header}\\n')\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text_line_items = []\n",
    "        for name, values in row.items():\n",
    "            if values.is_integer():\n",
    "                text_line_items.append(f\"{name}: {int(values)} \")\n",
    "            else:\n",
    "                text_line_items.append(f\"{name}: {values} \")\n",
    "            \n",
    "        text_line_items[-1] = text_line_items[-1].strip()\n",
    "        #text_line_items.append('\\n')\n",
    "        text_line = \"\".join(text_line_items) + \"\\n\"\n",
    "        #print(text_line)\n",
    "        representation.append(text_line)\n",
    "    representation.append(f'<|endofpiece|>\\n')\n",
    "\n",
    "    if filename:\n",
    "        assert(filename.endswith('.txt'))\n",
    "        with open(filename, 'w') as file:\n",
    "            file.writelines(representation)\n",
    "            \n",
    "    return \"\".join(representation)\n",
    "#a = tokenize_midi(df, \"Composer: Debussy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareAllData():\n",
    "    root_path = './inputs/final/'\n",
    "    with open(MIDI_TEXT_FILENAME, \"w\") as file:\n",
    "        file.write(\"<|startoftext|>\\n\")\n",
    "        for root, dirs, files in os.walk(root_path, topdown=False):\n",
    "            composer = root.split('/')[-1]\n",
    "            for filename in tqdm(files, desc=f\"Processing {composer:12}\"):\n",
    "                temp_path = os.path.join(root, filename)\n",
    "                df = pd.read_csv(temp_path)\n",
    "                file.write(tokenize_midi(df, f\"Composer: {composer}\"))\n",
    "        file.write(\"<|endoftext|>\\n\")\n",
    "\n",
    "#prepareAllData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3gTDQt3VNqo"
   },
   "source": [
    "Here we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktVqn5b0VOoa",
    "outputId": "b0db262f-0c7f-4471-bae7-a1ac0e4ee14c"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def load_tf_dataset(file_path, tokenizer, block_size=128, chunk_size=1000000):\n",
    "    def chunk_generator():\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            while True:\n",
    "                chunk = f.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                yield chunk\n",
    "\n",
    "    def process_chunk(chunk):\n",
    "        tokens = tokenizer.tokenize(chunk)\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        for i in range(0, len(ids) - block_size + 1, block_size):\n",
    "            input_ids = ids[i:i+block_size]\n",
    "            labels = ids[i+1:i+block_size+1] # shifted by 1\n",
    "            labels = [-100 if token == tokenizer.pad_token_id else token for token in labels] # masking with -100\n",
    "\n",
    "            yield {\n",
    "                \"input_ids\": input_ids,#ids[i:i+block_size],\n",
    "                \"attention_mask\": [1] * block_size,\n",
    "                \"labels\": labels\n",
    "            }\n",
    "\n",
    "    def gen():\n",
    "        for chunk in tqdm(chunk_generator(), desc=\"Processing chunks\"):\n",
    "            yield from process_chunk(chunk)\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            {\n",
    "                \"input_ids\": tf.TensorSpec(shape=(block_size,), dtype=tf.int32),\n",
    "                \"attention_mask\": tf.TensorSpec(shape=(block_size,), dtype=tf.int32),\n",
    "                \"labels\": tf.TensorSpec(shape=(block_size,), dtype=tf.int32)\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = load_tf_dataset(MIDI_TEXT_FILENAME, tokenizer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyzBvavLRQEy",
    "outputId": "c20e9e6c-b05d-44bd-8342-86c2b687421e"
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('CPU'),tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fmYZs41_UZ_q",
    "outputId": "7865b3b3-5fa0-4a84-ac3b-08d693bd6a63"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Claude 3.5Sonnet to the rescue, maybe\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "from transformers import DataCollatorForLanguageModeling, DefaultDataCollator\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Initialize tokenizer and data collator\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\"\"\"\n",
    "#data_collator = DataCollatorForLanguageModeling(\n",
    "#    tokenizer=tokenizer,\n",
    "#    mlm=True # changed from False\n",
    "#)\n",
    "\"\"\"\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# Training configuration\n",
    "class TrainingConfig:\n",
    "    output_dir = \"./gpt2-midi\"\n",
    "    num_train_epochs = 3\n",
    "    per_device_train_batch_size = 4\n",
    "    save_steps = 10_000\n",
    "    save_total_limit = 2\n",
    "    logging_dir = './logs'\n",
    "    logging_steps = 500\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "os.makedirs(config.logging_dir, exist_ok=True)\n",
    "\n",
    "# Set up distributed training strategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "# Create the model within the strategy scope\n",
    "with strategy.scope():\n",
    "    model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    \n",
    "    # Set up the optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=model.compute_loss\n",
    "    )\n",
    "\n",
    "# Prepare the dataset\n",
    "def prepare_dataset_for_training(dataset, batch_size):\n",
    "    \"\"\"Convert dataset to tf.data.Dataset format\"\"\"\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    \"\"\"\n",
    "    # Modify attention mask shape\n",
    "    dataset = dataset.map(lambda x: {\n",
    "        **x,\n",
    "        \"attention_mask\": tf.reshape(x[\"attention_mask\"], (tf.shape(x[\"attention_mask\"])[0], 1, 1, tf.shape(x[\"attention_mask\"])[2]))  # Reshape to (batch_size, 1, 1, seq_len)\n",
    "    })\n",
    "    \"\"\"\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = prepare_dataset_for_training(\n",
    "    train_dataset,  # Your original train_dataset\n",
    "    config.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "# Custom callback for model saving\n",
    "class ModelCheckpoint(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, save_steps, output_dir, save_total_limit):\n",
    "        super().__init__()\n",
    "        self.save_steps = save_steps\n",
    "        self.output_dir = output_dir\n",
    "        self.save_total_limit = save_total_limit\n",
    "        self.step = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.step += 1\n",
    "        if self.step % self.save_steps == 0:\n",
    "            # Save the model\n",
    "            checkpoint_dir = os.path.join(\n",
    "                self.output_dir,\n",
    "                f\"checkpoint-{self.step}\"\n",
    "            )\n",
    "            self.model.save_pretrained(checkpoint_dir)\n",
    "\n",
    "            # Remove old checkpoints if exceeding limit\n",
    "            checkpoints = sorted([\n",
    "                d for d in os.listdir(self.output_dir)\n",
    "                if d.startswith(\"checkpoint-\")\n",
    "            ])\n",
    "            while len(checkpoints) > self.save_total_limit:\n",
    "                checkpoint_to_remove = os.path.join(\n",
    "                    self.output_dir,\n",
    "                    checkpoints.pop(0)\n",
    "                )\n",
    "                tf.io.gfile.rmtree(checkpoint_to_remove)\n",
    "\n",
    "# Set up callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        save_steps=config.save_steps,\n",
    "        output_dir=config.output_dir,\n",
    "        save_total_limit=config.save_total_limit\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=config.logging_dir,\n",
    "        update_freq=config.logging_steps\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=config.num_train_epochs,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save the final model and tokenizer\n",
    "model.save_pretrained(config.output_dir)\n",
    "tokenizer.save_pretrained(config.output_dir)\n",
    "print(f\"Model saved to {config.output_dir}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "transformers.utils.logging.set_verbosity(transformers.logging.INFO)\n",
    "transformers.utils.logging.enable_progress_bar\n",
    "\n",
    "def fine_tune_gpt2(training_file, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    # Load tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    # Add special tokens if they're not already there\n",
    "    special_tokens = {\n",
    "        'additional_special_tokens': ['<|startofpiece|>', '<|endofpiece|>']\n",
    "    }\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    \n",
    "    # Load model\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2', torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=training_file,\n",
    "        block_size=128  # Adjust based on your data\n",
    "    )\n",
    "\n",
    "    print(\"Dataset loaded\")\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False  # Not using masked language modeling\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=100,\n",
    "        logging_first_step=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        \n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        save_steps=1000,\n",
    "        #fp16=True,\n",
    "        gradient_accumulation_steps=8,\n",
    "        save_total_limit=1,\n",
    "    )\n",
    "\n",
    "    print(\"Device is\", training_args.device)\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    print(\"Start Training\")\n",
    "    \n",
    "    # Train\n",
    "    trainer.train()    \n",
    "\n",
    "    print(\"Training done, saving to\", output_dir)\n",
    "    \n",
    "    # Save\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /home/paul-dutton/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json\n",
      "loading file merges.txt from cache at /home/paul-dutton/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/paul-dutton/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /home/paul-dutton/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /home/paul-dutton/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/paul-dutton/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/paul-dutton/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "Instantiating GPT2LMHeadModel model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/paul-dutton/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50259. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul-dutton/miniconda3/lib/python3.12/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Loading features from cached file ./inputs/cached_lm_GPT2Tokenizer_128_midi_text_data.txt [took 1.279 s]\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n",
      "Device is cuda:0\n",
      "Start Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 460,017\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 28,751\n",
      "  Number of trainable parameters = 124,441,344\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempting to unscale FP16 gradients.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfine_tune_gpt2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMIDI_TEXT_FILENAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./modeloutput/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 73\u001b[0m, in \u001b[0;36mfine_tune_gpt2\u001b[0;34m(training_file, output_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining done, saving to\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_dir)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Save\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2566\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2561\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   2562\u001b[0m         amp\u001b[38;5;241m.\u001b[39mmaster_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer),\n\u001b[1;32m   2563\u001b[0m         args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   2564\u001b[0m     )\n\u001b[1;32m   2565\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2566\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2572\u001b[0m     is_accelerate_available()\n\u001b[1;32m   2573\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2574\u001b[0m ):\n\u001b[1;32m   2575\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_global_grad_norm()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/accelerate/accelerator.py:2609\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2607\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m parameters \u001b[38;5;241m==\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()]:\n\u001b[1;32m   2608\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mclip_grad_norm_(max_norm, norm_type)\n\u001b[0;32m-> 2609\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[38;5;241m=\u001b[39mnorm_type)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/accelerate/accelerator.py:2548\u001b[0m, in \u001b[0;36mAccelerator.unscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(opt, AcceleratedOptimizer):\n\u001b[1;32m   2547\u001b[0m     opt \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39moptimizer\n\u001b[0;32m-> 2548\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:338\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    335\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    336\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 338\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unscale_grads_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:260\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m allow_fp16) \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to unscale FP16 gradients.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# coalesce() deduplicates indices and adds all values that have the same index.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# For scaled fp16 values, there's a good chance coalescing will cause overflow,\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# so we should check the coalesced _values().\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "\u001b[0;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."
     ]
    }
   ],
   "source": [
    "fine_tune_gpt2(MIDI_TEXT_FILENAME, './modeloutput/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(model_path, prompt=\"Composer: Bach\", max_length=512, num_return_sequences=1, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate music using your fine-tuned GPT-2 model\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the fine-tuned model\n",
    "        prompt: Text prompt to start generation (e.g., \"Composer: Bach\")\n",
    "        max_length: Maximum length of generated sequence\n",
    "        num_return_sequences: Number of different sequences to generate\n",
    "        temperature: Controls randomness (lower = more deterministic)\n",
    "        \n",
    "    Returns:\n",
    "        List of generated MIDI objects\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    \n",
    "    # Format the prompt with the right structure\n",
    "    formatted_prompt = f\"<|startofpiece|>\\n{prompt}\\n\"\n",
    "    \n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(formatted_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate continuation\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    # Decode and parse the output\n",
    "    generated_sequences = []\n",
    "    for sequence in output_sequences:\n",
    "        text = tokenizer.decode(sequence, skip_special_tokens=False)\n",
    "        \n",
    "        # Make sure we stop at the end of piece token if present\n",
    "        if \"<|endofpiece|>\" in text:\n",
    "            text = text.split(\"<|endofpiece|>\")[0] + \"<|endofpiece|>\"\n",
    "            \n",
    "        generated_sequences.append(text)\n",
    "        \n",
    "    # Convert text to MIDI\n",
    "    midi_objects = [text_to_midi(seq) for seq in generated_sequences]\n",
    "    \n",
    "    return midi_objects, generated_sequences\n",
    "\n",
    "midi_objects, generated_sequences = generate_music"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMBu5O++ygoL0a8AYABGRuy",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
