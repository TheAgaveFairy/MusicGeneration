{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBu5O++ygoL0a8AYABGRuy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheAgaveFairy/MusicGeneration/blob/main/gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/arnavmishra6996/music-generation-using-decoder-only-model-gpt2\n",
        "\n",
        "this is the inspiration, more or less copying it but in my own typing + modifications so i understand it"
      ],
      "metadata": {
        "id": "yTPZ89mS-YuW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8RS4Y2uM-V0e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pretty_midi\n",
        "import pretty_midi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1EOr3TZIqjA",
        "outputId": "b7e08ece-b31f-4d65-df21-bb0e1bd21f07"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pretty_midi in /usr/local/lib/python3.11/dist-packages (0.2.10)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.26.4)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mido>=1.1.16->pretty_midi) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s67Avc-NmK1G",
        "outputId": "12b18e43-dd7a-48d2-aacf-7c8a0daa86a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "#! kaggle datasets list\n",
        "\n",
        "! kaggle competitions download -c 'name-of-competition'\n",
        "! mkdir train\n",
        "! unzip train.zip -d train\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "zsxqUBpd_EiK",
        "outputId": "709e9fe7-7f01-48f3-c1c8-8054be7da58d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n!pip install -q kaggle\\nfrom google.colab import files\\nfiles.upload()\\n\\n! mkdir ~/.kaggle\\n\\n! cp kaggle.json ~/.kaggle/\\n! chmod 600 ~/.kaggle/kaggle.json\\n#! kaggle datasets list\\n\\n! kaggle competitions download -c 'name-of-competition'\\n! mkdir train\\n! unzip train.zip -d train\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"imsparsh/musicnet-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af71SasB_ncv",
        "outputId": "56640d32-1b23-4631-be13-c898a1f4dee1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/imsparsh/musicnet-dataset/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"for dirname, _, filenames in os.walk(path):\n",
        "    for filename in filenames[:10]: # just make sure this is working\n",
        "        print(os.path.join(dirname, filename))\n",
        "\"\"\"\n",
        "def list_files(startpath, trunc=True):\n",
        "    print(f\"root path is:\\n{path}\\nbeginning walk:\")\n",
        "    for root, dirs, files in os.walk(startpath):\n",
        "        level = root.replace(startpath, '').count(os.sep)\n",
        "        indent = ' ' * 4 * (level)\n",
        "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "        subindent = ' ' * 4 * (level + 1)\n",
        "        if trunc:\n",
        "            files = files[:2]\n",
        "        for f in files:\n",
        "            print('{}{}'.format(subindent, f))\n",
        "list_files(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4Kjkp0-Dl9e",
        "outputId": "257847e0-6855-4bee-cbdc-be8f971d8d04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root path is:\n",
            "/root/.cache/kagglehub/datasets/imsparsh/musicnet-dataset/versions/1\n",
            "beginning walk:\n",
            "1/\n",
            "    musicnet_metadata.csv\n",
            "    musicnet.npz\n",
            "    musicnet_midis/\n",
            "        musicnet_midis/\n",
            "            Ravel/\n",
            "                2178_gr_rqtf2.mid\n",
            "                2179_gr_rqtf3.mid\n",
            "            Cambini/\n",
            "                2080_quint2d3.mid\n",
            "                2078_quint2d1.mid\n",
            "            Beethoven/\n",
            "                2336_vns08_3.mid\n",
            "                2376_qt08_1.mid\n",
            "            Schubert/\n",
            "                1730_schubert_op114_5.mid\n",
            "                1758_d958-2.mid\n",
            "            Mozart/\n",
            "                1791_kv_465_4.mid\n",
            "                1859_kv_464_2.mid\n",
            "            Faure/\n",
            "                2166_gr_f45m1.mid\n",
            "                2167_gr_f45m2.mid\n",
            "            Bach/\n",
            "                2239_fugue17.mid\n",
            "                2284_vhs1_3.mid\n",
            "            Haydn/\n",
            "                2104_op64n5_1.mid\n",
            "                2105_op64n5_2.mid\n",
            "            Brahms/\n",
            "                2159_bra40_2.mid\n",
            "                2151_br25m4.mid\n",
            "            Dvorak/\n",
            "                1916_dvq10m1.mid\n",
            "                1919_dvqt10m4.mid\n",
            "    musicnet/\n",
            "        musicnet/\n",
            "            train_labels/\n",
            "                2345.csv\n",
            "                2296.csv\n",
            "            test_labels/\n",
            "                2382.csv\n",
            "                2303.csv\n",
            "            train_data/\n",
            "                2389.wav\n",
            "                1828.wav\n",
            "            test_data/\n",
            "                1759.wav\n",
            "                2382.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_file_path = os.path.join(path, 'musicnet/musicnet/test_labels/2382.csv')\n",
        "#print(temp_file_path)\n",
        "data = pd.read_csv(temp_file_path)\n",
        "data.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "pviCatj0G06u",
        "outputId": "5d1ea998-ca6c-4d34-8e5b-e103fe0e1d16"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   start_time  end_time  instrument  note  start_beat  end_beat     note_value\n",
              "0       10206     42462          41    61         0.0  1.489583    Dotted Half\n",
              "1       10206     25054          41    65         0.0  0.489583        Quarter\n",
              "2       10206     22494          43    46         0.0  0.333333  Dotted Eighth"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fe6e5416-1b7f-4f89-8474-43d45128657c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>instrument</th>\n",
              "      <th>note</th>\n",
              "      <th>start_beat</th>\n",
              "      <th>end_beat</th>\n",
              "      <th>note_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10206</td>\n",
              "      <td>42462</td>\n",
              "      <td>41</td>\n",
              "      <td>61</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.489583</td>\n",
              "      <td>Dotted Half</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10206</td>\n",
              "      <td>25054</td>\n",
              "      <td>41</td>\n",
              "      <td>65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.489583</td>\n",
              "      <td>Quarter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10206</td>\n",
              "      <td>22494</td>\n",
              "      <td>43</td>\n",
              "      <td>46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>Dotted Eighth</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe6e5416-1b7f-4f89-8474-43d45128657c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fe6e5416-1b7f-4f89-8474-43d45128657c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fe6e5416-1b7f-4f89-8474-43d45128657c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e4b3f0db-2657-4468-9f52-797108dac9a2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e4b3f0db-2657-4468-9f52-797108dac9a2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e4b3f0db-2657-4468-9f52-797108dac9a2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1956,\n  \"fields\": [\n    {\n      \"column\": \"start_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1323200,\n        \"min\": 10206,\n        \"max\": 5188574,\n        \"num_unique_values\": 840,\n        \"samples\": [\n          3954142,\n          4590046,\n          200669\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1323377,\n        \"min\": 22494,\n        \"max\": 5190622,\n        \"num_unique_values\": 1190,\n        \"samples\": [\n          1584094,\n          1130974,\n          1168862\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"instrument\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 41,\n        \"max\": 43,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          41,\n          43,\n          42\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"note\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 37,\n        \"max\": 94,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          68,\n          89,\n          38\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start_beat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 111.30368940526402,\n        \"min\": 0.0,\n        \"max\": 382.25,\n        \"num_unique_values\": 841,\n        \"samples\": [\n          216.5,\n          361.75,\n          11.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end_beat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27133420966447475,\n        \"min\": 0.114583333333,\n        \"max\": 4.98958333333,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          1.48958333333,\n          0.489583333333,\n          0.125\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"note_value\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Unknown\",\n          \"Quarter\",\n          \"Dotted Quarter\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's just pick a file and see what we're dealing with\n",
        "midi_data = pretty_midi.PrettyMIDI(os.path.join(path,'musicnet_midis/musicnet_midis/Beethoven/2313_qt15_1.mid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqO1XG4CI08G",
        "outputId": "eecdf3e9-1d82-450d-c8a1-00ae74fb8555"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "data = []\n",
        "for instrument in midi_data.instruments:\n",
        "    #print(instrument) # Instrument(program=40, is_drum=False, name=\"Violin1\") kinda output\n",
        "    for note in instrument.notes:\n",
        "        #print(note) # start, end, pitch, velocity\n",
        "        velocity = note.velocity\n",
        "        start_time = note.start\n",
        "        end_time = note.end\n",
        "        instrument_name = instrument.name if instrument.name else 'Unnamed'\n",
        "        note_name = pretty_midi.note_number_to_name(note.pitch)\n",
        "        start_beat = midi_data.get_beats(start_time)\n",
        "        end_beat = midi_data.get_beats(end_time)\n",
        "        note_value = note.pitch\n",
        "\n",
        "        data.append([start_time, end_time, instrument_name, note_name, start_beat, end_beat, velocity, note_value])\n",
        "\n",
        "# Create a DataFrame\n",
        "columns = ['start_time', 'end_time', 'instrument', 'note', 'start_beat', 'end_beat', 'velocity', 'note_value']\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# Save the DataFrame to a CSV file (optional)\n",
        "#df.to_csv('midi_data.csv', index=False)\n",
        "\n",
        "print(df.head())\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "QixPU3cSK77i",
        "outputId": "8ab425e8-36d5-41dc-d261-8ac6e3ea3315"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndata = []\\nfor instrument in midi_data.instruments:\\n    #print(instrument) # Instrument(program=40, is_drum=False, name=\"Violin1\") kinda output\\n    for note in instrument.notes:\\n        #print(note) # start, end, pitch, velocity\\n        velocity = note.velocity\\n        start_time = note.start\\n        end_time = note.end\\n        instrument_name = instrument.name if instrument.name else \\'Unnamed\\'\\n        note_name = pretty_midi.note_number_to_name(note.pitch)\\n        start_beat = midi_data.get_beats(start_time)\\n        end_beat = midi_data.get_beats(end_time)\\n        note_value = note.pitch\\n\\n        data.append([start_time, end_time, instrument_name, note_name, start_beat, end_beat, velocity, note_value])\\n\\n# Create a DataFrame\\ncolumns = [\\'start_time\\', \\'end_time\\', \\'instrument\\', \\'note\\', \\'start_beat\\', \\'end_beat\\', \\'velocity\\', \\'note_value\\']\\ndf = pd.DataFrame(data, columns=columns)\\n\\n# Save the DataFrame to a CSV file (optional)\\n#df.to_csv(\\'midi_data.csv\\', index=False)\\n\\nprint(df.head())\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entire_data = []\n",
        "from tqdm import tqdm\n",
        "def midi_data_extraction(file_path):\n",
        "    # Check if the path is a directory\n",
        "    if os.path.isdir(file_path):\n",
        "        # Loop through all files in the directory\n",
        "        for file_name in tqdm(os.listdir(file_path)):\n",
        "            if file_name.endswith('.mid'):\n",
        "                # Construct the full file path\n",
        "                full_path = os.path.join(file_path, file_name)\n",
        "                # Load the MIDI file\n",
        "                midi_data = pretty_midi.PrettyMIDI(full_path)\n",
        "\n",
        "                # Extract data for each note\n",
        "                for instrument in midi_data.instruments:\n",
        "                    for note in instrument.notes:\n",
        "                        start_time = note.start\n",
        "                        end_time = note.end\n",
        "                        instrument_name = instrument.name if instrument.name else 'Unnamed'\n",
        "                        note_name = pretty_midi.note_number_to_name(note.pitch)\n",
        "                        start_beat = midi_data.get_beats(start_time)\n",
        "                        end_beat = midi_data.get_beats(end_time)\n",
        "                        note_value = note.pitch\n",
        "                        duration = end_time - start_time  # Duration of the note\n",
        "                        velocity = note.velocity  # Velocity of the note\n",
        "\n",
        "                        # Add the data to the list\n",
        "                        entire_data.append([start_time, end_time, instrument_name, note_name, start_beat, end_beat, note_value, duration, velocity])"
      ],
      "metadata": {
        "id": "qGkwIoZ0Q93H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path = '/root/.cache/kagglehub/datasets/imsparsh/musicnet-dataset/versions/1'\n",
        "file_name = os.path.join(path, 'musicnet_midis/musicnet_midis/Beethoven')\n",
        "# print(file_name)\n",
        "midi_data_extraction(file_name)\n",
        "entire_data[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzMLz12kRPK9",
        "outputId": "74628123-c3c4-4b8f-c936-933f3219384a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  warnings.warn(\n",
            "100%|██████████| 157/157 [46:39<00:00, 17.83s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3.749994,\n",
              "  3.9019034791666667,\n",
              "  'Violin',\n",
              "  'D5',\n",
              "  array([  3.749994 ,   4.16666  ,   4.583326 ,   4.999992 ,   5.416658 ,\n",
              "           5.833324 ,   6.24999  ,   6.666656 ,   7.083322 ,   7.499988 ,\n",
              "           7.916654 ,   8.33332  ,   8.749986 ,   9.166652 ,   9.595223 ,\n",
              "          10.04658  ,  10.508118 ,  10.924784 ,  11.34145  ,  11.758116 ,\n",
              "          12.174782 ,  12.591448 ,  13.008114 ,  13.42478  ,  13.841446 ,\n",
              "          14.258112 ,  14.674778 ,  15.091444 ,  15.50811  ,  15.924776 ,\n",
              "          16.353347 ,  16.804704 ,  17.266242 ,  17.682908 ,  18.099574 ,\n",
              "          18.51624  ,  18.932906 ,  19.349572 ,  19.766238 ,  20.182904 ,\n",
              "          20.59957  ,  21.016236 ,  21.432902 ,  21.849568 ,  22.266234 ,\n",
              "          22.6829   ,  23.099566 ,  23.537732 ,  23.985493 ,  24.402159 ,\n",
              "          24.818825 ,  25.235491 ,  25.652157 ,  26.068823 ,  26.485489 ,\n",
              "          26.902155 ,  27.318821 ,  27.735487 ,  28.152153 ,  28.568819 ,\n",
              "          28.985485 ,  29.402151 ,  29.818817 ,  30.256983 ,  30.6891965,\n",
              "          31.1058625,  31.5225285,  31.9391945,  32.3558605,  32.7725265,\n",
              "          33.1891925,  33.6058585,  34.0225245,  34.4391905,  34.8558565,\n",
              "          35.2725225,  35.6891885,  36.1058545,  36.5225205,  36.9391865,\n",
              "          37.3558525,  37.7725185,  38.1891845,  38.6058505,  39.0225165,\n",
              "          39.4391825,  39.8677535,  40.3191105,  40.7806485,  41.1973145,\n",
              "          41.6139805,  42.0306465,  42.4473125,  42.8639785,  43.2806445,\n",
              "          43.6973105,  44.1139765,  44.5306425,  44.9473085,  45.3639745,\n",
              "          45.7806405,  46.1973065,  46.6139725,  47.0521385,  47.4998995,\n",
              "          47.9165655,  48.3332315,  48.7498975,  49.1665635,  49.5832295,\n",
              "          49.9998955,  50.4165615,  50.8332275,  51.2498935,  51.6665595,\n",
              "          52.0832255,  52.4998915,  52.9165575,  53.3332235,  53.7498895,\n",
              "          54.1665555,  54.5832215,  55.0117925,  55.4529685,  55.8941445,\n",
              "          56.3353205,  56.7764965,  57.2176725,  57.6588485,  58.1000245,\n",
              "          58.5412005,  58.9823765,  59.4235525,  59.8780975,  60.3468475,\n",
              "          60.7757685,  61.1924345,  61.6091005,  62.0257665,  62.4424325,\n",
              "          62.8590985,  63.2757645,  63.6924305,  64.1090965,  64.5257625,\n",
              "          64.9424285,  65.3590945,  65.7757605,  66.1924265,  66.6090925,\n",
              "          67.0257585,  67.4424245,  67.8590905,  68.2757565,  68.6924225,\n",
              "          69.1090885,  69.5376595,  69.9890165,  70.4505545,  70.8672205,\n",
              "          71.2838865,  71.7005525,  72.1172185,  72.5338845,  72.9505505,\n",
              "          73.3672165,  73.7838825,  74.2005485,  74.6172145,  75.0338805,\n",
              "          75.4505465,  75.8672125,  76.2838785,  76.7254365,  77.1799815,\n",
              "          77.5966475,  78.0133135,  78.4299795,  78.8466455,  79.2633115,\n",
              "          79.6799775,  80.0966435,  80.5133095,  80.9299755,  81.3466415,\n",
              "          81.7633075,  82.1799735,  82.5966395,  83.0133055,  83.4299715,\n",
              "          83.8466375,  84.2633035,  84.6799695,  85.0966355,  85.5133015,\n",
              "          85.9299675,  86.3466335,  86.7632995,  87.1799655,  87.5966315,\n",
              "          88.0132975,  88.4299635,  88.8466295,  89.2632955,  89.6799615,\n",
              "          90.0966275,  90.5132935,  90.9299595,  91.3466255,  91.7632915,\n",
              "          92.1799575,  92.5966235,  93.0132895,  93.4299555,  93.8466215,\n",
              "          94.2632875,  94.6799535,  95.0966195,  95.5132855,  95.9299515,\n",
              "          96.3466175,  96.7632835,  97.1799495,  97.5966155,  98.0132815,\n",
              "          98.4299475,  98.8466135,  99.2632795,  99.6799455, 100.0966115,\n",
              "         100.5132775, 100.9299435, 101.3466095, 101.7632755, 102.1799415,\n",
              "         102.5966075, 103.0251785, 103.4765355, 103.9156375, 104.3323035,\n",
              "         104.7489695, 105.1656355, 105.5823015, 105.9989675, 106.4156335,\n",
              "         106.8322995, 107.2489655, 107.6656315, 108.0822975, 108.5204635,\n",
              "         108.9586295, 109.3872005, 109.8157715, 110.2443425, 110.6729135,\n",
              "         111.1014845, 111.5300555, 111.9586265, 112.3871975, 112.8157685,\n",
              "         113.2443395, 113.6729105, 114.1014815, 114.5300525, 114.9586235,\n",
              "         115.3871945, 115.8157655, 116.2443365, 116.6729075, 117.1014785,\n",
              "         117.5300495, 117.9586205, 118.3871915, 118.8157625, 119.2443335,\n",
              "         119.6729045, 120.1014755, 120.5300465, 120.9586175, 121.3871885,\n",
              "         121.8157595, 122.2443305, 122.6729015, 123.1014725, 123.5300435,\n",
              "         123.9586145, 124.3871855, 124.8157565, 125.2443275, 125.6728985,\n",
              "         126.1014695, 126.5300405, 126.9586115, 127.3871825, 127.8157535,\n",
              "         128.2443245, 128.6855005, 129.1582045, 129.6420745, 130.0587405,\n",
              "         130.4754065, 130.8920725, 131.3087385, 131.7254045, 132.1420705,\n",
              "         132.5587365, 132.9754025, 133.3920685, 133.8087345, 134.2254005,\n",
              "         134.6420665, 135.0587325, 135.4753985, 135.8920645, 136.3087305,\n",
              "         136.7253965, 137.1420625, 137.5587285, 137.9753945, 138.3920605,\n",
              "         138.8206315, 139.2719885, 139.6886545, 140.1053205, 140.5219865,\n",
              "         140.9386525, 141.3553185, 141.7719845, 142.1886505, 142.6053165,\n",
              "         143.0219825, 143.4386485, 143.8553145, 144.2719805, 144.6886465,\n",
              "         145.1053125, 145.5219785, 145.9386445, 146.3672155, 146.8083915,\n",
              "         147.2629365, 147.7510315, 148.2510315, 148.7510315, 149.2510315,\n",
              "         149.7510315, 150.2510315, 150.7510315, 151.2510315, 151.7510315,\n",
              "         152.2510315, 152.7510315, 153.205681 , 153.640463 , 154.075245 ,\n",
              "         154.510027 , 154.944809 , 155.379591 , 155.814373 , 156.249155 ,\n",
              "         156.683937 , 157.118719 , 157.553501 , 157.988283 , 158.423065 ,\n",
              "         158.857847 , 159.292629 , 159.727411 , 160.162193 , 160.596975 ,\n",
              "         161.031757 , 161.466539 , 161.901321 , 162.336103 , 162.770885 ,\n",
              "         163.199456 , 163.628027 , 164.056598 , 164.485169 , 164.91374  ,\n",
              "         165.342311 , 165.770882 , 166.199453 , 166.628024 , 167.056595 ,\n",
              "         167.485166 , 167.913737 , 168.342308 , 168.770879 , 169.222236 ,\n",
              "         169.6672905, 170.0958615, 170.5244325, 170.9530035, 171.3815745,\n",
              "         171.8101455, 172.2387165, 172.6672875, 173.0958585, 173.5244295,\n",
              "         173.9530005, 174.4043575, 174.849412 , 175.277983 , 175.706554 ,\n",
              "         176.135125 , 176.563696 , 176.992267 , 177.420838 , 177.849409 ,\n",
              "         178.27798  , 178.706551 , 179.135122 , 179.563693 , 179.992264 ,\n",
              "         180.420835 , 180.849406 , 181.277977 , 181.706548 , 182.135119 ,\n",
              "         182.56369  , 182.992261 , 183.420832 , 183.849403 , 184.277974 ,\n",
              "         184.706545 , 185.135116 , 185.563687 , 185.992258 , 186.420829 ,\n",
              "         186.843364 , 187.265899 , 187.682565 , 188.099231 , 188.515897 ,\n",
              "         188.932563 , 189.361134 , 189.812491 , 190.274029 , 190.829584 ,\n",
              "         191.385139 , 191.940694 , 192.496249 , 193.051804 , 193.607359 ,\n",
              "         194.162914 ]),\n",
              "  array([  3.90190348,   4.31856948,   4.73523548,   5.15190148,\n",
              "           5.56856748,   5.98523348,   6.40189948,   6.81856548,\n",
              "           7.23523148,   7.65189748,   8.06856348,   8.48522948,\n",
              "           8.90189548,   9.32290184,   9.75606842,  10.21484906,\n",
              "          10.66002748,  11.07669348,  11.49335948,  11.91002548,\n",
              "          12.32669148,  12.74335748,  13.16002348,  13.57668948,\n",
              "          13.99335548,  14.41002148,  14.82668748,  15.24335348,\n",
              "          15.66001948,  16.08102584,  16.51419242,  16.97297306,\n",
              "          17.41815148,  17.83481748,  18.25148348,  18.66814948,\n",
              "          19.08481548,  19.50148148,  19.91814748,  20.33481348,\n",
              "          20.75147948,  21.16814548,  21.58481148,  22.00147748,\n",
              "          22.41814348,  22.83480948,  23.25581584,  23.7009782 ,\n",
              "          24.13740248,  24.55406848,  24.97073448,  25.38740048,\n",
              "          25.80406648,  26.22073248,  26.63739848,  27.05406448,\n",
              "          27.47073048,  27.88739648,  28.30406248,  28.72072848,\n",
              "          29.13739448,  29.55406048,  29.97506684,  30.4202292 ,\n",
              "          30.84110598,  31.25777198,  31.67443798,  32.09110398,\n",
              "          32.50776998,  32.92443598,  33.34110198,  33.75776798,\n",
              "          34.17443398,  34.59109998,  35.00776598,  35.42443198,\n",
              "          35.84109798,  36.25776398,  36.67442998,  37.09109598,\n",
              "          37.50776198,  37.92442798,  38.34109398,  38.75775998,\n",
              "          39.17442598,  39.59543234,  40.02859892,  40.48737956,\n",
              "          40.93255798,  41.34922398,  41.76588998,  42.18255598,\n",
              "          42.59922198,  43.01588798,  43.43255398,  43.84921998,\n",
              "          44.26588598,  44.68255198,  45.09921798,  45.51588398,\n",
              "          45.93254998,  46.34921598,  46.77022234,  47.2153847 ,\n",
              "          47.65180898,  48.06847498,  48.48514098,  48.90180698,\n",
              "          49.31847298,  49.73513898,  50.15180498,  50.56847098,\n",
              "          50.98513698,  51.40180298,  51.81846898,  52.23513498,\n",
              "          52.65180098,  53.06846698,  53.48513298,  53.90179898,\n",
              "          54.31846498,  54.73947134,  55.17263792,  55.61381392,\n",
              "          56.05498992,  56.49616592,  56.93734192,  57.37851792,\n",
              "          57.81969392,  58.26086992,  58.70204592,  59.14322192,\n",
              "          59.58927203,  60.04899594,  60.50769292,  60.92767798,\n",
              "          61.34434398,  61.76100998,  62.17767598,  62.59434198,\n",
              "          63.01100798,  63.42767398,  63.84433998,  64.26100598,\n",
              "          64.67767198,  65.09433798,  65.51100398,  65.92766998,\n",
              "          66.34433598,  66.76100198,  67.17766798,  67.59433398,\n",
              "          68.01099998,  68.42766598,  68.84433198,  69.26533834,\n",
              "          69.69850492,  70.15728556,  70.60246398,  71.01912998,\n",
              "          71.43579598,  71.85246198,  72.26912798,  72.68579398,\n",
              "          73.10245998,  73.51912598,  73.93579198,  74.35245798,\n",
              "          74.76912398,  75.18578998,  75.60245598,  76.01912198,\n",
              "          76.44012834,  76.89115603,  77.33189098,  77.74855698,\n",
              "          78.16522298,  78.58188898,  78.99855498,  79.41522098,\n",
              "          79.83188698,  80.24855298,  80.66521898,  81.08188498,\n",
              "          81.49855098,  81.91521698,  82.33188298,  82.74854898,\n",
              "          83.16521498,  83.58188098,  83.99854698,  84.41521298,\n",
              "          84.83187898,  85.24854498,  85.66521098,  86.08187698,\n",
              "          86.49854298,  86.91520898,  87.33187498,  87.74854098,\n",
              "          88.16520698,  88.58187298,  88.99853898,  89.41520498,\n",
              "          89.83187098,  90.24853698,  90.66520298,  91.08186898,\n",
              "          91.49853498,  91.91520098,  92.33186698,  92.74853298,\n",
              "          93.16519898,  93.58186498,  93.99853098,  94.41519698,\n",
              "          94.83186298,  95.24852898,  95.66519498,  96.08186098,\n",
              "          96.49852698,  96.91519298,  97.33185898,  97.74852498,\n",
              "          98.16519098,  98.58185698,  98.99852298,  99.41518898,\n",
              "          99.83185498, 100.24852098, 100.66518698, 101.08185298,\n",
              "         101.49851898, 101.91518498, 102.33185098, 102.75285734,\n",
              "         103.18602392, 103.64480456, 104.06754698, 104.48421298,\n",
              "         104.90087898, 105.31754498, 105.73421098, 106.15087698,\n",
              "         106.56754298, 106.98420898, 107.40087498, 107.81754098,\n",
              "         108.23854734, 108.6837097 , 109.11487934, 109.54345034,\n",
              "         109.97202134, 110.40059234, 110.82916334, 111.25773434,\n",
              "         111.68630534, 112.11487634, 112.54344734, 112.97201834,\n",
              "         113.40058934, 113.82916034, 114.25773134, 114.68630234,\n",
              "         115.11487334, 115.54344434, 115.97201534, 116.40058634,\n",
              "         116.82915734, 117.25772834, 117.68629934, 118.11487034,\n",
              "         118.54344134, 118.97201234, 119.40058334, 119.82915434,\n",
              "         120.25772534, 120.68629634, 121.11486734, 121.54343834,\n",
              "         121.97200934, 122.40058034, 122.82915134, 123.25772234,\n",
              "         123.68629334, 124.11486434, 124.54343534, 124.97200634,\n",
              "         125.40057734, 125.82914834, 126.25771934, 126.68629034,\n",
              "         127.11486134, 127.54343234, 127.97200334, 128.40516992,\n",
              "         128.85376956, 129.33461544, 129.79398398, 130.21064998,\n",
              "         130.62731598, 131.04398198, 131.46064798, 131.87731398,\n",
              "         132.29397998, 132.71064598, 133.12731198, 133.54397798,\n",
              "         133.96064398, 134.37730998, 134.79397598, 135.21064198,\n",
              "         135.62730798, 136.04397398, 136.46063998, 136.87730598,\n",
              "         137.29397198, 137.71063798, 138.12730398, 138.54831034,\n",
              "         138.98147692, 139.42389798, 139.84056398, 140.25722998,\n",
              "         140.67389598, 141.09056198, 141.50722798, 141.92389398,\n",
              "         142.34055998, 142.75722598, 143.17389198, 143.59055798,\n",
              "         144.00722398, 144.42388998, 144.84055598, 145.25722198,\n",
              "         145.67388798, 146.09489434, 146.52806092, 146.97411103,\n",
              "         147.43654744, 147.93332317, 148.43332317, 148.93332317,\n",
              "         149.43332317, 149.93332317, 150.43332317, 150.93332317,\n",
              "         151.43332317, 151.93332317, 152.43332317, 152.91930056,\n",
              "         153.36419527, 153.79897727, 154.23375927, 154.66854127,\n",
              "         155.10332327, 155.53810527, 155.97288727, 156.40766927,\n",
              "         156.84245127, 157.27723327, 157.71201527, 158.14679727,\n",
              "         158.58157927, 159.01636127, 159.45114327, 159.88592527,\n",
              "         160.32070727, 160.75548927, 161.19027127, 161.62505327,\n",
              "         162.05983527, 162.49461727, 162.92713484, 163.35570584,\n",
              "         163.78427684, 164.21284784, 164.64141884, 165.06998984,\n",
              "         165.49856084, 165.92713184, 166.35570284, 166.78427384,\n",
              "         167.21284484, 167.64141584, 168.06998684, 168.49855784,\n",
              "         168.93172442, 169.39050506, 169.82354034, 170.25211134,\n",
              "         170.68068234, 171.10925334, 171.53782434, 171.96639534,\n",
              "         172.39496634, 172.82353734, 173.25210834, 173.68067934,\n",
              "         174.11384592, 174.57262656, 175.00566184, 175.43423284,\n",
              "         175.86280384, 176.29137484, 176.71994584, 177.14851684,\n",
              "         177.57708784, 178.00565884, 178.43422984, 178.86280084,\n",
              "         179.29137184, 179.71994284, 180.14851384, 180.57708484,\n",
              "         181.00565584, 181.43422684, 181.86279784, 182.29136884,\n",
              "         182.71993984, 183.14851084, 183.57708184, 184.00565284,\n",
              "         184.43422384, 184.86279484, 185.29136584, 185.71993684,\n",
              "         186.14850784, 186.57487822, 186.99741322, 187.41780848,\n",
              "         187.83447448, 188.25114048, 188.66780648, 189.08881284,\n",
              "         189.52197942, 189.98076006, 190.47657509, 191.03213009,\n",
              "         191.58768509, 192.14324009, 192.69879509, 193.25435009,\n",
              "         193.80990509, 194.36546009]),\n",
              "  74,\n",
              "  0.15190947916666664,\n",
              "  70]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['start_time', 'end_time', 'instrument', 'note', 'start_beat', 'end_beat', 'note_value', 'duration', 'velocity']\n",
        "\n",
        "# Create a DataFrame with the collected data\n",
        "df = pd.DataFrame(entire_data, columns=columns)\n",
        "\n",
        "# Save the DataFrame to a CSV file (optional)\n",
        "#df.to_csv('entire_midi_data.csv', index=False)\n",
        "\n",
        "# Print the first few rows of the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j0HMdiCU7Oy",
        "outputId": "f5ba0b5a-f0fe-4a9b-c046-be1813aaaed4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [start_time, end_time, instrument, note, start_beat, end_beat, note_value, duration, velocity]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 needs this as text"
      ],
      "metadata": {
        "id": "c4dFQv20VCyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('midi_text_data.txt', 'w') as file:\n",
        "    file.write('<|startoftext|>\\n')\n",
        "    for index, row in tqdm(df.iterrows()):\n",
        "        text_line = f\"start_time: {row['start_time']}, end_time: {row['end_time']}, instrument: {row['instrument']}, note: {row['note']}, start_beat: {row['start_beat']}, end_beat: {row['end_beat']}, note_value: {row['note_value']}, duration: {row['duration']}, velocity: {row['velocity']}\\n\"\n",
        "        file.write(text_line)\n",
        "    file.write('<|endoftext|>\\n')"
      ],
      "metadata": {
        "id": "zJaDtaAyVB8V"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we go"
      ],
      "metadata": {
        "id": "G3gTDQt3VNqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def load_tf_dataset(file_path, tokenizer, block_size=128, chunk_size=1000000):\n",
        "    def chunk_generator():\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            while True:\n",
        "                chunk = f.read(chunk_size)\n",
        "                if not chunk:\n",
        "                    break\n",
        "                yield chunk\n",
        "\n",
        "    def process_chunk(chunk):\n",
        "        tokens = tokenizer.tokenize(chunk)\n",
        "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        for i in range(0, len(ids) - block_size + 1, block_size):\n",
        "            input_ids = ids[i:i+block_size]\n",
        "            labels = ids[i+1:i+block_size+1] # shifted by 1\n",
        "            labels = [-100 if token == tokenizer.pad_token_id else token for token in labels] # masking with -100\n",
        "\n",
        "            yield {\n",
        "                \"input_ids\": input_ids,#ids[i:i+block_size],\n",
        "                \"attention_mask\": [1] * block_size,\n",
        "                \"labels\": labels\n",
        "            }\n",
        "\n",
        "    def gen():\n",
        "        for chunk in tqdm(chunk_generator(), desc=\"Processing chunks\"):\n",
        "            yield from process_chunk(chunk)\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        output_signature=(\n",
        "            {\n",
        "                \"input_ids\": tf.TensorSpec(shape=(block_size,), dtype=tf.int32),\n",
        "                \"attention_mask\": tf.TensorSpec(shape=(block_size,), dtype=tf.int32),\n",
        "                \"labels\": tf.TensorSpec(shape=(block_size,), dtype=tf.int32)\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = load_tf_dataset('midi_text_data.txt', tokenizer)"
      ],
      "metadata": {
        "id": "ktVqn5b0VOoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0db262f-0c7f-4471-bae7-a1ac0e4ee14c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers[tf-cpu]"
      ],
      "metadata": {
        "id": "3siVjqslmhF7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!mkdir \"/content/drive/My Drive/MusicGen\""
      ],
      "metadata": {
        "id": "YvjYI4oKLnK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f910a8-031a-4158-d342-fe01c2c8068f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "mkdir: cannot create directory ‘/content/drive/My Drive/MusicGen’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "with open('/content/drive/My Drive/MusicGen/midi_text_data.txt', 'w') as f:\n",
        "  with open('midi_text_data.txt', 'r') as f2:\n",
        "    f.write(f2.read())\n",
        "  #f.write('./midi_text_data.txt')\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "24xorWXVKrBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices('CPU'),tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "id": "oyzBvavLRQEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20e9e6c-b05d-44bd-8342-86c2b687421e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')], [])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, TFTrainingArguments #TFTrainer,\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import TFTrainer# as TFTrainer\n",
        "from tqdm import tqdm\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir=\"./gpt2-midi\",  # Directory where the model and tokenizer will be saved\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    per_device_train_batch_size=4,  # Batch size for training\n",
        "    save_steps=10_000,  # Save every 10,000 steps\n",
        "    save_total_limit=2,  # Keep only the 2 most recent models\n",
        "    prediction_loss_only=True,  # Only compute the loss during training\n",
        "    logging_dir='./logs',  # Directory for logs\n",
        "    logging_steps=500  # Log every 500 steps\n",
        ")\n",
        "\n",
        "#Gemini\n",
        "strategy = tf.distribute.MirroredStrategy(['CPU:0'])\n",
        "\n",
        "# Create the model within the strategy scope\n",
        "with strategy.scope():\n",
        "    model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    #/Gemini\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = TFTrainer(\n",
        "    model=model,#TFGPT2LMHeadModel.from_pretrained('gpt2'),\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset\n",
        ")\n",
        "\"\"\"\n",
        "'This code was deprecated on or about 2021.'"
      ],
      "metadata": {
        "id": "Jm3TC7mjl9rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the model and tokenizer\n",
        "trainer.save_model('./gpt2-midi')\n",
        "tokenizer.save_pretrained('./gpt2-midi')\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nmNXNj64l-R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Claude 3.5Sonnet to the rescue, maybe\n",
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "from transformers import DataCollatorForLanguageModeling, DefaultDataCollator\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Initialize tokenizer and data collator\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\"\"\"\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True # changed from False\n",
        ")\n",
        "\"\"\"\n",
        "data_collator = DefaultDataCollator()\n",
        "\n",
        "# Training configuration\n",
        "class TrainingConfig:\n",
        "    output_dir = \"./gpt2-midi\"\n",
        "    num_train_epochs = 3\n",
        "    per_device_train_batch_size = 4\n",
        "    save_steps = 10_000\n",
        "    save_total_limit = 2\n",
        "    logging_dir = './logs'\n",
        "    logging_steps = 500\n",
        "\n",
        "config = TrainingConfig()\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(config.output_dir, exist_ok=True)\n",
        "os.makedirs(config.logging_dir, exist_ok=True)\n",
        "\n",
        "# Set up distributed training strategy\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
        "\n",
        "# Create the model within the strategy scope\n",
        "with strategy.scope():\n",
        "    model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    # Set up the optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=model.compute_loss\n",
        "    )\n",
        "\n",
        "# Prepare the dataset\n",
        "def prepare_dataset_for_training(dataset, batch_size):\n",
        "    \"\"\"Convert dataset to tf.data.Dataset format\"\"\"\n",
        "    dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "    \"\"\"\n",
        "    # Modify attention mask shape\n",
        "    dataset = dataset.map(lambda x: {\n",
        "        **x,\n",
        "        \"attention_mask\": tf.reshape(x[\"attention_mask\"], (tf.shape(x[\"attention_mask\"])[0], 1, 1, tf.shape(x[\"attention_mask\"])[2]))  # Reshape to (batch_size, 1, 1, seq_len)\n",
        "    })\n",
        "    \"\"\"\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = prepare_dataset_for_training(\n",
        "    train_dataset,  # Your original train_dataset\n",
        "    config.per_device_train_batch_size\n",
        ")\n",
        "\n",
        "# Custom callback for model saving\n",
        "class ModelCheckpoint(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, save_steps, output_dir, save_total_limit):\n",
        "        super().__init__()\n",
        "        self.save_steps = save_steps\n",
        "        self.output_dir = output_dir\n",
        "        self.save_total_limit = save_total_limit\n",
        "        self.step = 0\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        self.step += 1\n",
        "        if self.step % self.save_steps == 0:\n",
        "            # Save the model\n",
        "            checkpoint_dir = os.path.join(\n",
        "                self.output_dir,\n",
        "                f\"checkpoint-{self.step}\"\n",
        "            )\n",
        "            self.model.save_pretrained(checkpoint_dir)\n",
        "\n",
        "            # Remove old checkpoints if exceeding limit\n",
        "            checkpoints = sorted([\n",
        "                d for d in os.listdir(self.output_dir)\n",
        "                if d.startswith(\"checkpoint-\")\n",
        "            ])\n",
        "            while len(checkpoints) > self.save_total_limit:\n",
        "                checkpoint_to_remove = os.path.join(\n",
        "                    self.output_dir,\n",
        "                    checkpoints.pop(0)\n",
        "                )\n",
        "                tf.io.gfile.rmtree(checkpoint_to_remove)\n",
        "\n",
        "# Set up callbacks\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        save_steps=config.save_steps,\n",
        "        output_dir=config.output_dir,\n",
        "        save_total_limit=config.save_total_limit\n",
        "    ),\n",
        "    tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=config.logging_dir,\n",
        "        update_freq=config.logging_steps\n",
        "    )\n",
        "]\n",
        "\n",
        "# Training\n",
        "print(\"Starting training...\")\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=config.num_train_epochs,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Save the final model and tokenizer\n",
        "model.save_pretrained(config.output_dir)\n",
        "tokenizer.save_pretrained(config.output_dir)\n",
        "print(f\"Model saved to {config.output_dir}\")"
      ],
      "metadata": {
        "id": "fmYZs41_UZ_q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7865b3b3-5fa0-4a84-ac3b-08d693bd6a63"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of devices: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1381, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 1672, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filebt088adg.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_filewows8e8x.py\", line 34, in tf__call\n        transformer_outputs = ag__.converted_call(ag__.ld(self).transformer, (), dict(input_ids=ag__.ld(input_ids), past_key_values=ag__.ld(past_key_values), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), encoder_hidden_states=ag__.ld(encoder_hidden_states), encoder_attention_mask=ag__.ld(encoder_attention_mask), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_filebt088adg.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_filedpzt6o0g.py\", line 116, in tf__call\n        ag__.if_stmt(ag__.ld(attention_mask) is not None, if_body_5, else_body_5, get_state_5, set_state_5, ('attention_mask',), 1)\n    File \"/tmp/__autograph_generated_filedpzt6o0g.py\", line 106, in if_body_5\n        attention_mask = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(attention_mask), (ag__.ld(attention_mask_shape)[0], 1, 1, ag__.ld(attention_mask_shape)[1])), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tfgpt2lm_head_model_3' (type TFGPT2LMHeadModel).\n    \n    in user code:\n    \n        File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 890, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 922, in call  *\n            transformer_outputs = self.transformer(\n        File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_filebt088adg.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"/tmp/__autograph_generated_filedpzt6o0g.py\", line 116, in tf__call\n            ag__.if_stmt(ag__.ld(attention_mask) is not None, if_body_5, else_body_5, get_state_5, set_state_5, ('attention_mask',), 1)\n        File \"/tmp/__autograph_generated_filedpzt6o0g.py\", line 106, in if_body_5\n            attention_mask = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(attention_mask), (ag__.ld(attention_mask_shape)[0], 1, 1, ag__.ld(attention_mask_shape)[1])), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'transformer' (type TFGPT2MainLayer).\n        \n        in user code:\n        \n            File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 890, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 439, in call  *\n                attention_mask = tf.reshape(attention_mask, (attention_mask_shape[0], 1, 1, attention_mask_shape[1]))\n        \n            ValueError: Cannot reshape a tensor with 8192 elements to shape [4,1,1,4] (16 elements) for '{{node tfgpt2lm_head_model_3/transformer/Reshape_1}} = Reshape[T=DT_INT32, Tshape=DT_INT32](IteratorGetNext, tfgpt2lm_head_model_3/transformer/Reshape_1/shape)' with input shapes: [4,4,4,128], [4] and with input tensors computed as partial shapes: input[1] = [4,1,1,4].\n        \n        \n        Call arguments received by layer 'transformer' (type TFGPT2MainLayer):\n          • input_ids=tf.Tensor(shape=(4, 4, 4, 128), dtype=int32)\n          • past_key_values=None\n          • attention_mask=tf.Tensor(shape=(4, 4, 4, 128), dtype=int32)\n          • token_type_ids=None\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • use_cache=True\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer 'tfgpt2lm_head_model_3' (type TFGPT2LMHeadModel):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(4, 4, 4, 128), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(4, 4, 4, 128), dtype=int32)', 'labels': 'tf.Tensor(shape=(4, 4, 4, 128), dtype=int32)'}\n      • past_key_values=None\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • encoder_hidden_states=None\n      • encoder_attention_mask=None\n      • use_cache=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d5b9573f6683>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_batch_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__train_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1670\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1673\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_using_dummy_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mtf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__run_call_with_unpacked_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mtransformer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_...\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mtf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__run_call_with_unpacked_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mone_cst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'one_cst'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mattention_mask_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attention_mask_shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mget_state_8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\u001b[0m in \u001b[0;36mif_body_5\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0;32mnonlocal\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mattention_mask_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                     \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                     \u001b[0mone_cst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_cst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1381, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 1672, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filebt088adg.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_filewows8e8x.py\", line 34, in tf__call\n        transformer_outputs = ag__.converted_call(ag__.ld(self).transformer, (), dict(input_ids=ag__.ld(input_ids), past_key_values=ag__.ld(past_key_values), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), encoder_hidden_states=ag__.ld(encoder_hidden_states), encoder_attention_mask=ag__.ld(encoder_attention_mask), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_filebt088adg.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_filedpzt6o0g.py\", line 116, in tf__call\n        ag__.if_stmt(ag__.ld(attention_mask) is not None, if_body_5, else_body_5, get_state_5, set_state_5, ('attention_mask',), 1)\n    File \"/tmp/__autograph_generated_filedpzt6o0g.py\", line 106, in if_body_5\n        attention_mask = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(attention_mask), (ag__.ld(attention_mask_shape)[0], 1, 1, ag__.ld(attention_mask_shape)[1])), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tfgpt2lm_head_model_3' (type TFGPT2LMHeadModel).\n    \n    in user code:\n    \n        File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 890, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 922, in call  *\n            transformer_outputs = self.transformer(\n        File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_filebt088adg.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"/tmp/__autograph_generated_filedpzt6o0g.py\", line 116, in tf__call\n            ag__.if_stmt(ag__.ld(attention_mask) is not None, if_body_5, else_body_5, get_state_5, set_state_5, ('attention_mask',), 1)\n        File \"/tmp/__autograph_generated_filedpzt6o0g.py\", line 106, in if_body_5\n            attention_mask = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(attention_mask), (ag__.ld(attention_mask_shape)[0], 1, 1, ag__.ld(attention_mask_shape)[1])), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'transformer' (type TFGPT2MainLayer).\n        \n        in user code:\n        \n            File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 890, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 439, in call  *\n                attention_mask = tf.reshape(attention_mask, (attention_mask_shape[0], 1, 1, attention_mask_shape[1]))\n        \n            ValueError: Cannot reshape a tensor with 8192 elements to shape [4,1,1,4] (16 elements) for '{{node tfgpt2lm_head_model_3/transformer/Reshape_1}} = Reshape[T=DT_INT32, Tshape=DT_INT32](IteratorGetNext, tfgpt2lm_head_model_3/transformer/Reshape_1/shape)' with input shapes: [4,4,4,128], [4] and with input tensors computed as partial shapes: input[1] = [4,1,1,4].\n        \n        \n        Call arguments received by layer 'transformer' (type TFGPT2MainLayer):\n          • input_ids=tf.Tensor(shape=(4, 4, 4, 128), dtype=int32)\n          • past_key_values=None\n          • attention_mask=tf.Tensor(shape=(4, 4, 4, 128), dtype=int32)\n          • token_type_ids=None\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • use_cache=True\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer 'tfgpt2lm_head_model_3' (type TFGPT2LMHeadModel):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(4, 4, 4, 128), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(4, 4, 4, 128), dtype=int32)', 'labels': 'tf.Tensor(shape=(4, 4, 4, 128), dtype=int32)'}\n      • past_key_values=None\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • encoder_hidden_states=None\n      • encoder_attention_mask=None\n      • use_cache=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n"
          ]
        }
      ]
    }
  ]
}