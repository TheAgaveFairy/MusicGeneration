{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TheAgaveFairy/MusicGeneration/blob/main/gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTPZ89mS-YuW"
   },
   "source": [
    "https://www.kaggle.com/code/arnavmishra6996/music-generation-using-decoder-only-model-gpt2\n",
    "\n",
    "this is the inspiration, more or less copying it but in my own typing + modifications so i understand it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8RS4Y2uM-V0e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1EOr3TZIqjA",
    "outputId": "b7e08ece-b31f-4d65-df21-bb0e1bd21f07"
   },
   "outputs": [],
   "source": [
    "#! pip install pretty_midi\n",
    "import pretty_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s67Avc-NmK1G",
    "outputId": "12b18e43-dd7a-48d2-aacf-7c8a0daa86a9"
   },
   "outputs": [],
   "source": [
    "#! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "zsxqUBpd_EiK",
    "outputId": "709e9fe7-7f01-48f3-c1c8-8054be7da58d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n!pip install -q kaggle\\nfrom google.colab import files\\nfiles.upload()\\n\\n! mkdir ~/.kaggle\\n\\n! cp kaggle.json ~/.kaggle/\\n! chmod 600 ~/.kaggle/kaggle.json\\n#! kaggle datasets list\\n\\n! kaggle competitions download -c 'name-of-competition'\\n! mkdir train\\n! unzip train.zip -d train\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!pip install -q kaggle\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "\n",
    "! mkdir ~/.kaggle\n",
    "\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "#! kaggle datasets list\n",
    "\n",
    "! kaggle competitions download -c 'name-of-competition'\n",
    "! mkdir train\n",
    "! unzip train.zip -d train\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Af71SasB_ncv",
    "outputId": "56640d32-1b23-4631-be13-c898a1f4dee1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/imsparsh/musicnet-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 21.5G/21.5G [36:26<00:00, 10.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/paul-dutton/.cache/kagglehub/datasets/imsparsh/musicnet-dataset/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"imsparsh/musicnet-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "'Path to dataset files: /home/paul-dutton/.cache/kagglehub/datasets/imsparsh/musicnet-dataset/versions/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4Kjkp0-Dl9e",
    "outputId": "257847e0-6855-4bee-cbdc-be8f971d8d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root path is:\n",
      "/home/paul-dutton/.cache/kagglehub/datasets/imsparsh/musicnet-dataset/versions/1\n",
      "beginning walk:\n",
      "1/\n",
      "    musicnet.npz\n",
      "    musicnet_metadata.csv\n",
      "    musicnet_midis/\n",
      "        musicnet_midis/\n",
      "            Faure/\n",
      "                2166_gr_f45m1.mid\n",
      "                2168_gr_f45m3.mid\n",
      "            Bach/\n",
      "                2243_vs1_3.mid\n",
      "                2659_vs2_6.mid\n",
      "            Dvorak/\n",
      "                1932_dv96_3.mid\n",
      "                1919_dvqt10m4.mid\n",
      "            Cambini/\n",
      "                2082_quint3f2.mid\n",
      "                2077_quintBb3.mid\n",
      "            Haydn/\n",
      "                2105_op64n5_2.mid\n",
      "                2104_op64n5_1.mid\n",
      "            Brahms/\n",
      "                2151_br25m4.mid\n",
      "                2148_br25m1.mid\n",
      "            Mozart/\n",
      "                1819_k3754a.mid\n",
      "                1811_kv581b.mid\n",
      "            Ravel/\n",
      "                2180_gr_rqtf4.mid\n",
      "                2179_gr_rqtf3.mid\n",
      "            Beethoven/\n",
      "                2510_ps02_02.mid\n",
      "                2322_ps23_01.mid\n",
      "            Schubert/\n",
      "                1757_d958-1.mid\n",
      "                1735_sy_sps94.mid\n",
      "    musicnet/\n",
      "        musicnet/\n",
      "            train_labels/\n",
      "                2373.csv\n",
      "                2359.csv\n",
      "            train_data/\n",
      "                2443.wav\n",
      "                2566.wav\n",
      "            test_data/\n",
      "                2106.wav\n",
      "                2298.wav\n",
      "            test_labels/\n",
      "                2191.csv\n",
      "                2382.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"for dirname, _, filenames in os.walk(path):\n",
    "    for filename in filenames[:10]: # just make sure this is working\n",
    "        print(os.path.join(dirname, filename))\n",
    "\"\"\"\n",
    "def list_files(startpath, trunc=True):\n",
    "    print(f\"root path is:\\n{path}\\nbeginning walk:\")\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        if trunc:\n",
    "            files = files[:2]\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))\n",
    "list_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "pviCatj0G06u",
    "outputId": "5d1ea998-ca6c-4d34-8e5b-e103fe0e1d16"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>instrument</th>\n",
       "      <th>note</th>\n",
       "      <th>start_beat</th>\n",
       "      <th>end_beat</th>\n",
       "      <th>note_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10206</td>\n",
       "      <td>42462</td>\n",
       "      <td>41</td>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.489583</td>\n",
       "      <td>Dotted Half</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10206</td>\n",
       "      <td>25054</td>\n",
       "      <td>41</td>\n",
       "      <td>65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.489583</td>\n",
       "      <td>Quarter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10206</td>\n",
       "      <td>22494</td>\n",
       "      <td>43</td>\n",
       "      <td>46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Dotted Eighth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_time  end_time  instrument  note  start_beat  end_beat     note_value\n",
       "0       10206     42462          41    61         0.0  1.489583    Dotted Half\n",
       "1       10206     25054          41    65         0.0  0.489583        Quarter\n",
       "2       10206     22494          43    46         0.0  0.333333  Dotted Eighth"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_file_path = os.path.join(path, 'musicnet/musicnet/test_labels/2382.csv')\n",
    "#print(temp_file_path)\n",
    "data = pd.read_csv(temp_file_path)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uqO1XG4CI08G",
    "outputId": "eecdf3e9-1d82-450d-c8a1-00ae74fb8555"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul-dutton/miniconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# let's just pick a file and see what we're dealing with\n",
    "midi_data = pretty_midi.PrettyMIDI(os.path.join(path,'musicnet_midis/musicnet_midis/Beethoven/2313_qt15_1.mid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "QixPU3cSK77i",
    "outputId": "8ab425e8-36d5-41dc-d261-8ac6e3ea3315"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\ndata = []\\nfor instrument in midi_data.instruments:\\n    #print(instrument) # Instrument(program=40, is_drum=False, name=\"Violin1\") kinda output\\n    for note in instrument.notes:\\n        #print(note) # start, end, pitch, velocity\\n        velocity = note.velocity\\n        start_time = note.start\\n        end_time = note.end\\n        instrument_name = instrument.name if instrument.name else \\'Unnamed\\'\\n        note_name = pretty_midi.note_number_to_name(note.pitch)\\n        start_beat = midi_data.get_beats(start_time)\\n        end_beat = midi_data.get_beats(end_time)\\n        note_value = note.pitch\\n\\n        data.append([start_time, end_time, instrument_name, note_name, start_beat, end_beat, velocity, note_value])\\n\\n# Create a DataFrame\\ncolumns = [\\'start_time\\', \\'end_time\\', \\'instrument\\', \\'note\\', \\'start_beat\\', \\'end_beat\\', \\'velocity\\', \\'note_value\\']\\ndf = pd.DataFrame(data, columns=columns)\\n\\n# Save the DataFrame to a CSV file (optional)\\n#df.to_csv(\\'midi_data.csv\\', index=False)\\n\\nprint(df.head())\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "data = []\n",
    "for instrument in midi_data.instruments:\n",
    "    #print(instrument) # Instrument(program=40, is_drum=False, name=\"Violin1\") kinda output\n",
    "    for note in instrument.notes:\n",
    "        #print(note) # start, end, pitch, velocity\n",
    "        velocity = note.velocity\n",
    "        start_time = note.start\n",
    "        end_time = note.end\n",
    "        instrument_name = instrument.name if instrument.name else 'Unnamed'\n",
    "        note_name = pretty_midi.note_number_to_name(note.pitch)\n",
    "        start_beat = midi_data.get_beats(start_time)\n",
    "        end_beat = midi_data.get_beats(end_time)\n",
    "        note_value = note.pitch\n",
    "\n",
    "        data.append([start_time, end_time, instrument_name, note_name, start_beat, end_beat, velocity, note_value])\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = ['start_time', 'end_time', 'instrument', 'note', 'start_beat', 'end_beat', 'velocity', 'note_value']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Save the DataFrame to a CSV file (optional)\n",
    "#df.to_csv('midi_data.csv', index=False)\n",
    "\n",
    "print(df.head())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qGkwIoZ0Q93H"
   },
   "outputs": [],
   "source": [
    "entire_data = []\n",
    "from tqdm import tqdm\n",
    "def midi_data_extraction(file_path):\n",
    "    # Check if the path is a directory\n",
    "    if os.path.isdir(file_path):\n",
    "        # Loop through all files in the directory\n",
    "        for file_name in tqdm(os.listdir(file_path)):\n",
    "            if file_name.endswith('.mid'):\n",
    "                # Construct the full file path\n",
    "                full_path = os.path.join(file_path, file_name)\n",
    "                # Load the MIDI file\n",
    "                midi_data = pretty_midi.PrettyMIDI(full_path)\n",
    "\n",
    "                # Extract data for each note\n",
    "                for instrument in midi_data.instruments:\n",
    "                    for note in instrument.notes:\n",
    "                        start_time = note.start\n",
    "                        end_time = note.end\n",
    "                        instrument_name = instrument.name if instrument.name else 'Unnamed'\n",
    "                        note_name = pretty_midi.note_number_to_name(note.pitch)\n",
    "                        start_beat = midi_data.get_beats(start_time)\n",
    "                        end_beat = midi_data.get_beats(end_time)\n",
    "                        note_value = note.pitch\n",
    "                        duration = end_time - start_time  # Duration of the note\n",
    "                        velocity = note.velocity  # Velocity of the note\n",
    "\n",
    "                        # Add the data to the list\n",
    "                        entire_data.append([start_time, end_time, instrument_name, note_name, start_beat, end_beat, note_value, duration, velocity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzMLz12kRPK9",
    "outputId": "74628123-c3c4-4b8f-c936-933f3219384a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 157/157 [10:49<00:00,  4.14s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[np.float64(0.0),\n",
       "  np.float64(1.2369791666666667),\n",
       "  'Right',\n",
       "  'F#4',\n",
       "  array([  0.       ,   1.25     ,   2.5      ,   3.75     ,   5.       ,\n",
       "           6.25     ,   7.5      ,   8.75     ,  10.       ,  11.25     ,\n",
       "          12.5      ,  13.75     ,  15.       ,  16.25     ,  17.5      ,\n",
       "          18.75     ,  20.       ,  21.25     ,  22.5      ,  23.75     ,\n",
       "          25.       ,  26.25     ,  27.5      ,  28.75     ,  30.       ,\n",
       "          31.25     ,  32.5      ,  33.75     ,  35.       ,  36.25     ,\n",
       "          37.5      ,  38.75     ,  40.       ,  41.25     ,  42.5416665,\n",
       "          44.0251825,  45.767606 ,  47.017606 ,  48.267606 ,  49.517606 ,\n",
       "          50.767606 ,  52.017606 ,  53.267606 ,  54.517606 ,  55.767606 ,\n",
       "          57.017606 ,  58.267606 ,  59.517606 ,  60.767606 ,  62.017606 ,\n",
       "          63.267606 ,  64.517606 ,  65.767606 ,  67.017606 ,  68.267606 ,\n",
       "          69.517606 ,  70.671452 ,  71.825298 ,  72.979144 ,  74.13299  ,\n",
       "          75.286836 ,  76.440682 ,  77.594528 ,  78.748374 ,  79.90222  ,\n",
       "          81.056066 ,  82.146975 ,  83.237884 ,  84.328793 ,  85.419702 ,\n",
       "          86.510611 ,  87.60152  ,  88.692429 ,  89.783338 ,  90.874247 ,\n",
       "          91.965156 ,  93.165156 ,  94.365156 ,  95.565156 ,  96.765156 ,\n",
       "          97.965156 ,  99.165156 , 100.256065 , 101.346974 , 102.437883 ,\n",
       "         103.528792 , 104.619701 , 105.71061  , 106.801519 , 107.892428 ,\n",
       "         108.983337 , 110.2750035, 111.7585195, 113.500943 , 114.700943 ,\n",
       "         115.900943 , 117.100943 , 118.300943 , 119.500943 , 120.700943 ,\n",
       "         121.900943 , 123.100943 , 124.300943 , 125.500943 , 126.700943 ,\n",
       "         127.900943 , 129.100943 , 130.300943 , 131.500943 , 132.700943 ,\n",
       "         133.900943 , 135.100943 , 136.300943 , 137.500943 , 138.700943 ,\n",
       "         139.900943 , 141.100943 , 142.300943 , 143.500943 , 144.700943 ,\n",
       "         145.900943 , 147.100943 , 148.300943 , 149.500943 , 150.700943 ,\n",
       "         151.900943 , 153.100943 , 154.3642405, 155.6830805, 157.0625725,\n",
       "         158.2390425, 159.4155125, 160.5919825, 161.7684525, 162.9449225,\n",
       "         164.1213925, 165.2978625, 166.4743325, 167.6508025, 168.8272725,\n",
       "         170.0037425, 171.1802125, 172.3566825, 173.5331525, 174.7096225,\n",
       "         175.8860925, 177.0625625, 178.2390325, 179.4155025, 180.5919725,\n",
       "         181.7684425, 182.9449125, 184.1213825, 185.2978525, 186.4743225,\n",
       "         187.6507925, 188.8272625, 190.0037325, 191.1802025, 192.3566725,\n",
       "         193.5331425, 194.7096125, 195.8860825, 197.0625525, 198.2390225,\n",
       "         199.4154925, 200.5919625, 201.7684325, 202.9449025, 204.2082   ,\n",
       "         205.52704  , 206.906532 , 208.060378 , 209.214224 , 210.36807  ,\n",
       "         211.521916 , 212.675762 , 213.829608 , 214.983454 , 216.1373   ,\n",
       "         217.291146 , 218.444992 , 219.598838 , 220.752684 , 221.90653  ,\n",
       "         223.060376 , 224.214222 , 225.368068 , 226.521914 , 227.67576  ,\n",
       "         228.829606 , 229.983452 , 231.137298 , 232.291144 , 233.44499  ,\n",
       "         234.598836 , 235.752682 , 236.8956425, 238.0277175, 239.291015 ,\n",
       "         240.609855 , 242.2506525, 243.3415615, 244.4324705, 245.5233795,\n",
       "         246.6142885, 247.7051975, 248.7961065, 249.8870155, 250.9779245,\n",
       "         252.0688335, 253.1597425, 254.2506515, 255.3415605, 256.4324695,\n",
       "         257.5233785, 258.6142875, 259.7051965, 260.7961055, 261.8870145,\n",
       "         262.9779235, 264.0688325, 265.1597415, 266.3597415, 267.5597415,\n",
       "         268.7597415, 269.9597415, 271.1597415, 272.3597415, 273.5597415,\n",
       "         274.7597415, 275.9597415, 277.1597415, 278.3597415, 279.6930745,\n",
       "         281.1216455, 282.5502165, 284.319447 , 286.319447 , 288.319447 ,\n",
       "         290.319447 ]),\n",
       "  array([  1.23697917,   2.48697917,   3.73697917,   4.98697917,\n",
       "           6.23697917,   7.48697917,   8.73697917,   9.98697917,\n",
       "          11.23697917,  12.48697917,  13.73697917,  14.98697917,\n",
       "          16.23697917,  17.48697917,  18.73697917,  19.98697917,\n",
       "          21.23697917,  22.48697917,  23.73697917,  24.98697917,\n",
       "          26.23697917,  27.48697917,  28.73697917,  29.98697917,\n",
       "          31.23697917,  32.48697917,  33.73697917,  34.98697917,\n",
       "          36.23697917,  37.48697917,  38.73697917,  39.98697917,\n",
       "          41.23697917,  42.52777761,  44.00915686,  45.74866661,\n",
       "          47.00458517,  48.25458517,  49.50458517,  50.75458517,\n",
       "          52.00458517,  53.25458517,  54.50458517,  55.75458517,\n",
       "          57.00458517,  58.25458517,  59.50458517,  60.75458517,\n",
       "          62.00458517,  63.25458517,  64.50458517,  65.75458517,\n",
       "          67.00458517,  68.25458517,  69.50458517,  70.65943277,\n",
       "          71.81327877,  72.96712477,  74.12097077,  75.27481677,\n",
       "          76.42866277,  77.58250877,  78.73635477,  79.89020077,\n",
       "          81.04404677,  82.13561136,  83.22652036,  84.31742936,\n",
       "          85.40833836,  86.49924736,  87.59015636,  88.68106536,\n",
       "          89.77197436,  90.86288336,  91.95379236,  93.152656  ,\n",
       "          94.352656  ,  95.552656  ,  96.752656  ,  97.952656  ,\n",
       "          99.152656  , 100.24470136, 101.33561036, 102.42651936,\n",
       "         103.51742836, 104.60833736, 105.69924636, 106.79015536,\n",
       "         107.88106436, 108.97197336, 110.26111461, 111.74249386,\n",
       "         113.48200361, 114.688443  , 115.888443  , 117.088443  ,\n",
       "         118.288443  , 119.488443  , 120.688443  , 121.888443  ,\n",
       "         123.088443  , 124.288443  , 125.488443  , 126.688443  ,\n",
       "         127.888443  , 129.088443  , 130.288443  , 131.488443  ,\n",
       "         132.688443  , 133.888443  , 135.088443  , 136.288443  ,\n",
       "         137.488443  , 138.688443  , 139.888443  , 141.088443  ,\n",
       "         142.288443  , 143.488443  , 144.688443  , 145.888443  ,\n",
       "         147.088443  , 148.288443  , 149.488443  , 150.688443  ,\n",
       "         151.888443  , 153.088443  , 154.35094264, 155.66919161,\n",
       "         157.04803762, 158.2267876 , 159.4032576 , 160.5797276 ,\n",
       "         161.7561976 , 162.9326676 , 164.1091376 , 165.2856076 ,\n",
       "         166.4620776 , 167.6385476 , 168.8150176 , 169.9914876 ,\n",
       "         171.1679576 , 172.3444276 , 173.5208976 , 174.6973676 ,\n",
       "         175.8738376 , 177.0503076 , 178.2267776 , 179.4032476 ,\n",
       "         180.5797176 , 181.7561876 , 182.9326576 , 184.1091276 ,\n",
       "         185.2855976 , 186.4620676 , 187.6385376 , 188.8150076 ,\n",
       "         189.9914776 , 191.1679476 , 192.3444176 , 193.5208876 ,\n",
       "         194.6973576 , 195.8738276 , 197.0502976 , 198.2267676 ,\n",
       "         199.4032376 , 200.5797076 , 201.7561776 , 202.9326476 ,\n",
       "         204.19490214, 205.51315111, 206.89199712, 208.04835877,\n",
       "         209.20220477, 210.35605077, 211.50989677, 212.66374277,\n",
       "         213.81758877, 214.97143477, 216.12528077, 217.27912677,\n",
       "         218.43297277, 219.58681877, 220.74066477, 221.89451077,\n",
       "         223.04835677, 224.20220277, 225.35604877, 226.50989477,\n",
       "         227.66374077, 228.81758677, 229.97143277, 231.12527877,\n",
       "         232.27912477, 233.43297077, 234.58681677, 235.74066277,\n",
       "         236.88385005, 238.01592505, 239.27771714, 240.59596611,\n",
       "         242.23171311, 243.33019786, 244.42110686, 245.51201586,\n",
       "         246.60292486, 247.69383386, 248.78474286, 249.87565186,\n",
       "         250.96656086, 252.05746986, 253.14837886, 254.23928786,\n",
       "         255.33019686, 256.42110586, 257.51201486, 258.60292386,\n",
       "         259.69383286, 260.78474186, 261.87565086, 262.96655986,\n",
       "         264.05746886, 265.14837786, 266.3472415 , 267.5472415 ,\n",
       "         268.7472415 , 269.9472415 , 271.1472415 , 272.3472415 ,\n",
       "         273.5472415 , 274.7472415 , 275.9472415 , 277.1472415 ,\n",
       "         278.3472415 , 279.67918561, 281.10676455, 282.53533555,\n",
       "         284.29861367, 286.29861367, 288.29861367, 290.29861367,\n",
       "         292.29861367]),\n",
       "  66,\n",
       "  np.float64(1.2369791666666667),\n",
       "  80]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path = '/root/.cache/kagglehub/datasets/imsparsh/musicnet-dataset/versions/1'\n",
    "file_name = os.path.join(path, 'musicnet_midis/musicnet_midis/Beethoven')\n",
    "# print(file_name)\n",
    "midi_data_extraction(file_name)\n",
    "entire_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8j0HMdiCU7Oy",
    "outputId": "f5ba0b5a-f0fe-4a9b-c046-be1813aaaed4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   start_time  end_time instrument note  \\\n",
      "0        0.00  1.236979      Right  F#4   \n",
      "1        0.00  1.236979      Right   A3   \n",
      "2        1.25  2.486979      Right  F#4   \n",
      "3        1.25  2.486979      Right   A3   \n",
      "4        2.50  3.736979      Right  F#4   \n",
      "\n",
      "                                          start_beat  \\\n",
      "0  [0.0, 1.25, 2.5, 3.75, 5.0, 6.25, 7.5, 8.75, 1...   \n",
      "1  [0.0, 1.25, 2.5, 3.75, 5.0, 6.25, 7.5, 8.75, 1...   \n",
      "2  [1.25, 2.5, 3.75, 5.0, 6.25, 7.5, 8.75, 10.0, ...   \n",
      "3  [1.25, 2.5, 3.75, 5.0, 6.25, 7.5, 8.75, 10.0, ...   \n",
      "4  [2.5, 3.75, 5.0, 6.25, 7.5, 8.75, 10.0, 11.25,...   \n",
      "\n",
      "                                            end_beat  note_value  duration  \\\n",
      "0  [1.2369791666666667, 2.486979166666667, 3.7369...          66  1.236979   \n",
      "1  [1.2369791666666667, 2.486979166666667, 3.7369...          57  1.236979   \n",
      "2  [2.486979166666667, 3.736979166666667, 4.98697...          66  1.236979   \n",
      "3  [2.486979166666667, 3.736979166666667, 4.98697...          57  1.236979   \n",
      "4  [3.736979166666667, 4.986979166666667, 6.23697...          66  1.236979   \n",
      "\n",
      "   velocity  \n",
      "0        80  \n",
      "1        80  \n",
      "2        80  \n",
      "3        80  \n",
      "4        80  \n"
     ]
    }
   ],
   "source": [
    "columns = ['start_time', 'end_time', 'instrument', 'note', 'start_beat', 'end_beat', 'note_value', 'duration', 'velocity']\n",
    "\n",
    "# Create a DataFrame with the collected data\n",
    "df = pd.DataFrame(entire_data, columns=columns)\n",
    "\n",
    "# Save the DataFrame to a CSV file (optional)\n",
    "df.to_csv('entire_midi_data.csv', index=False)\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4dFQv20VCyI"
   },
   "source": [
    "GPT-2 needs this as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zJaDtaAyVB8V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "569506it [12:49, 740.49it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('midi_text_data.txt', 'w') as file:\n",
    "    file.write('<|startoftext|>\\n')\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        text_line = f\"start_time: {row['start_time']}, end_time: {row['end_time']}, instrument: {row['instrument']}, note: {row['note']}, start_beat: {row['start_beat']}, end_beat: {row['end_beat']}, note_value: {row['note_value']}, duration: {row['duration']}, velocity: {row['velocity']}\\n\"\n",
    "        file.write(text_line)\n",
    "    file.write('<|endoftext|>\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3gTDQt3VNqo"
   },
   "source": [
    "Here we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktVqn5b0VOoa",
    "outputId": "b0db262f-0c7f-4471-bae7-a1ac0e4ee14c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def load_tf_dataset(file_path, tokenizer, block_size=128, chunk_size=1000000):\n",
    "    def chunk_generator():\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            while True:\n",
    "                chunk = f.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                yield chunk\n",
    "\n",
    "    def process_chunk(chunk):\n",
    "        tokens = tokenizer.tokenize(chunk)\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        for i in range(0, len(ids) - block_size + 1, block_size):\n",
    "            input_ids = ids[i:i+block_size]\n",
    "            labels = ids[i+1:i+block_size+1] # shifted by 1\n",
    "            labels = [-100 if token == tokenizer.pad_token_id else token for token in labels] # masking with -100\n",
    "\n",
    "            yield {\n",
    "                \"input_ids\": input_ids,#ids[i:i+block_size],\n",
    "                \"attention_mask\": [1] * block_size,\n",
    "                \"labels\": labels\n",
    "            }\n",
    "\n",
    "    def gen():\n",
    "        for chunk in tqdm(chunk_generator(), desc=\"Processing chunks\"):\n",
    "            yield from process_chunk(chunk)\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            {\n",
    "                \"input_ids\": tf.TensorSpec(shape=(block_size,), dtype=tf.int32),\n",
    "                \"attention_mask\": tf.TensorSpec(shape=(block_size,), dtype=tf.int32),\n",
    "                \"labels\": tf.TensorSpec(shape=(block_size,), dtype=tf.int32)\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = load_tf_dataset('midi_text_data.txt', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3siVjqslmhF7"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers[tf-cpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvjYI4oKLnK6",
    "outputId": "14f910a8-031a-4158-d342-fe01c2c8068f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "mkdir: cannot create directory ‘/content/drive/My Drive/MusicGen’: File exists\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "!mkdir \"/content/drive/My Drive/MusicGen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24xorWXVKrBQ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open('/content/drive/My Drive/MusicGen/midi_text_data.txt', 'w') as f:\n",
    "  with open('midi_text_data.txt', 'r') as f2:\n",
    "    f.write(f2.read())\n",
    "  #f.write('./midi_text_data.txt')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyzBvavLRQEy",
    "outputId": "c20e9e6c-b05d-44bd-8342-86c2b687421e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')], [])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('CPU'),tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jm3TC7mjl9rV"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, TFTrainingArguments #TFTrainer,\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TFTrainer# as TFTrainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir=\"./gpt2-midi\",  # Directory where the model and tokenizer will be saved\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size for training\n",
    "    save_steps=10_000,  # Save every 10,000 steps\n",
    "    save_total_limit=2,  # Keep only the 2 most recent models\n",
    "    prediction_loss_only=True,  # Only compute the loss during training\n",
    "    logging_dir='./logs',  # Directory for logs\n",
    "    logging_steps=500  # Log every 500 steps\n",
    ")\n",
    "\n",
    "#Gemini\n",
    "strategy = tf.distribute.MirroredStrategy(['CPU:0'])\n",
    "\n",
    "# Create the model within the strategy scope\n",
    "with strategy.scope():\n",
    "    model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    #/Gemini\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = TFTrainer(\n",
    "    model=model,#TFGPT2LMHeadModel.from_pretrained('gpt2'),\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\"\"\"\n",
    "'This code was deprecated on or about 2021.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmNXNj64l-R5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "trainer.save_model('./gpt2-midi')\n",
    "tokenizer.save_pretrained('./gpt2-midi')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fmYZs41_UZ_q",
    "outputId": "7865b3b3-5fa0-4a84-ac3b-08d693bd6a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 19:14:39.445264: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: in user code:\n",
      "\n",
      "    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1588, in compute_loss  *\n",
      "        return super().compute_loss(*args, **kwargs)\n",
      "    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1206, in compute_loss  **\n",
      "        return self.compiled_loss(\n",
      "    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/compile_utils.py\", line 275, in __call__\n",
      "        y_t, y_p, sw = match_dtype_and_rank(y_t, y_p, sw)\n",
      "    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/compile_utils.py\", line 854, in match_dtype_and_rank\n",
      "        if (y_t.dtype.is_floating and y_p.dtype.is_floating) or (\n",
      "\n",
      "    AttributeError: 'NoneType' object has no attribute 'dtype'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/training/coordinator.py\", line 293, in stop_on_exception\n",
      "    yield\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 387, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 693, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 690, in wrapper\n",
      "    return converted_call(f, args, kwargs, options=options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 377, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 459, in _call_unconverted\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step\n",
      "    outputs = model.train_step(data)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1706, in train_step\n",
      "    loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/compile_utils.py\", line 277, in __call__\n",
      "    loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/losses.py\", line 143, in __call__\n",
      "    losses = call_fn(y_true, y_pred)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 693, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 690, in wrapper\n",
      "    return converted_call(f, args, kwargs, options=options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 331, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options, False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 459, in _call_unconverted\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/losses.py\", line 270, in call\n",
      "    return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 693, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 690, in wrapper\n",
      "    return converted_call(f, args, kwargs, options=options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 439, in converted_call\n",
      "    result = converted_f(*effective_args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/__autograph_generated_fileuftjev8f.py\", line 38, in tf__compute_loss\n",
      "    ag__.if_stmt(ag__.converted_call(ag__.ld(hasattr), (ag__.ld(keras).Model, 'compute_loss'), None, fscope), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 1217, in if_stmt\n",
      "    _py_if_stmt(cond, body, orelse)\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 1270, in _py_if_stmt\n",
      "    return body() if cond else orelse()\n",
      "           ^^^^^^\n",
      "  File \"/tmp/__autograph_generated_fileuftjev8f.py\", line 24, in if_body\n",
      "    retval_ = ag__.converted_call(ag__.converted_call(ag__.ld(super), (), None, fscope).compute_loss, tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 377, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 459, in _call_unconverted\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1206, in compute_loss\n",
      "    return self.compiled_loss(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/compile_utils.py\", line 275, in __call__\n",
      "    y_t, y_p, sw = match_dtype_and_rank(y_t, y_p, sw)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/compile_utils.py\", line 854, in match_dtype_and_rank\n",
      "    if (y_t.dtype.is_floating and y_p.dtype.is_floating) or (\n",
      "                                  ^^^^^^^^^\n",
      "AttributeError: in user code:\n",
      "\n",
      "    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1588, in compute_loss  *\n",
      "        return super().compute_loss(*args, **kwargs)\n",
      "    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1206, in compute_loss  **\n",
      "        return self.compiled_loss(\n",
      "    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/compile_utils.py\", line 275, in __call__\n",
      "        y_t, y_p, sw = match_dtype_and_rank(y_t, y_p, sw)\n",
      "    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/compile_utils.py\", line 854, in match_dtype_and_rank\n",
      "        if (y_t.dtype.is_floating and y_p.dtype.is_floating) or (\n",
      "\n",
      "    AttributeError: 'NoneType' object has no attribute 'dtype'\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1588, in compute_loss  *\n        return super().compute_loss(*args, **kwargs)\n    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1206, in compute_loss  **\n        return self.compiled_loss(\n    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/compile_utils.py\", line 275, in __call__\n        y_t, y_p, sw = match_dtype_and_rank(y_t, y_p, sw)\n    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/compile_utils.py\", line 854, in match_dtype_and_rank\n        if (y_t.dtype.is_floating and y_p.dtype.is_floating) or (\n\n    AttributeError: 'NoneType' object has no attribute 'dtype'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Save the final model and tokenizer\u001b[39;00m\n\u001b[1;32m    124\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(config\u001b[38;5;241m.\u001b[39moutput_dir)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filezd3sovmi.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:1706\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1703\u001b[0m             y_pred \u001b[38;5;241m=\u001b[39m y_pred[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1706\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularization_losses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[38;5;66;03m# Run backwards pass.\u001b[39;00m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mminimize(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables, tape\u001b[38;5;241m=\u001b[39mtape)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileuftjev8f.py:38\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcompute_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdo_return\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretval_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fscope\u001b[38;5;241m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileuftjev8f.py:24\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss.<locals>.if_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28msuper\u001b[39m), (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\u001b[38;5;241m.\u001b[39mcompute_loss, \u001b[38;5;28mtuple\u001b[39m(ag__\u001b[38;5;241m.\u001b[39mld(args)), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(kwargs)), fscope)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1588, in compute_loss  *\n        return super().compute_loss(*args, **kwargs)\n    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1206, in compute_loss  **\n        return self.compiled_loss(\n    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/compile_utils.py\", line 275, in __call__\n        y_t, y_p, sw = match_dtype_and_rank(y_t, y_p, sw)\n    File \"/home/paul-dutton/miniconda3/lib/python3.12/site-packages/tf_keras/src/engine/compile_utils.py\", line 854, in match_dtype_and_rank\n        if (y_t.dtype.is_floating and y_p.dtype.is_floating) or (\n\n    AttributeError: 'NoneType' object has no attribute 'dtype'\n"
     ]
    }
   ],
   "source": [
    "#Claude 3.5Sonnet to the rescue, maybe\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "from transformers import DataCollatorForLanguageModeling, DefaultDataCollator\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Initialize tokenizer and data collator\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\"\"\"\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True # changed from False\n",
    ")\n",
    "\"\"\"\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# Training configuration\n",
    "class TrainingConfig:\n",
    "    output_dir = \"./gpt2-midi\"\n",
    "    num_train_epochs = 3\n",
    "    per_device_train_batch_size = 4\n",
    "    save_steps = 10_000\n",
    "    save_total_limit = 2\n",
    "    logging_dir = './logs'\n",
    "    logging_steps = 500\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "os.makedirs(config.logging_dir, exist_ok=True)\n",
    "\n",
    "# Set up distributed training strategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "# Create the model within the strategy scope\n",
    "with strategy.scope():\n",
    "    model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    # Set up the optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=model.compute_loss\n",
    "    )\n",
    "\n",
    "# Prepare the dataset\n",
    "def prepare_dataset_for_training(dataset, batch_size):\n",
    "    \"\"\"Convert dataset to tf.data.Dataset format\"\"\"\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    \"\"\"\n",
    "    # Modify attention mask shape\n",
    "    dataset = dataset.map(lambda x: {\n",
    "        **x,\n",
    "        \"attention_mask\": tf.reshape(x[\"attention_mask\"], (tf.shape(x[\"attention_mask\"])[0], 1, 1, tf.shape(x[\"attention_mask\"])[2]))  # Reshape to (batch_size, 1, 1, seq_len)\n",
    "    })\n",
    "    \"\"\"\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = prepare_dataset_for_training(\n",
    "    train_dataset,  # Your original train_dataset\n",
    "    config.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "# Custom callback for model saving\n",
    "class ModelCheckpoint(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, save_steps, output_dir, save_total_limit):\n",
    "        super().__init__()\n",
    "        self.save_steps = save_steps\n",
    "        self.output_dir = output_dir\n",
    "        self.save_total_limit = save_total_limit\n",
    "        self.step = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.step += 1\n",
    "        if self.step % self.save_steps == 0:\n",
    "            # Save the model\n",
    "            checkpoint_dir = os.path.join(\n",
    "                self.output_dir,\n",
    "                f\"checkpoint-{self.step}\"\n",
    "            )\n",
    "            self.model.save_pretrained(checkpoint_dir)\n",
    "\n",
    "            # Remove old checkpoints if exceeding limit\n",
    "            checkpoints = sorted([\n",
    "                d for d in os.listdir(self.output_dir)\n",
    "                if d.startswith(\"checkpoint-\")\n",
    "            ])\n",
    "            while len(checkpoints) > self.save_total_limit:\n",
    "                checkpoint_to_remove = os.path.join(\n",
    "                    self.output_dir,\n",
    "                    checkpoints.pop(0)\n",
    "                )\n",
    "                tf.io.gfile.rmtree(checkpoint_to_remove)\n",
    "\n",
    "# Set up callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        save_steps=config.save_steps,\n",
    "        output_dir=config.output_dir,\n",
    "        save_total_limit=config.save_total_limit\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=config.logging_dir,\n",
    "        update_freq=config.logging_steps\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=config.num_train_epochs,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save the final model and tokenizer\n",
    "model.save_pretrained(config.output_dir)\n",
    "tokenizer.save_pretrained(config.output_dir)\n",
    "print(f\"Model saved to {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMBu5O++ygoL0a8AYABGRuy",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
